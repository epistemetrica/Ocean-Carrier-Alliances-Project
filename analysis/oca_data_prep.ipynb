{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation for Ocean Carrier Alliances Project\n",
    "\n",
    "This notebook loads, cleans, and prepares the data used to analyse strategic alliances among containerized maritime freight carriers, which we call Ocean Carrier Alliances (OCAs). The primary data comes from S&P's [PIERS BOL database](https://www.spglobal.com/marketintelligence/en/mi/products/piers.html), which is processed via the seperate PIERS Data Project. To the PIERS data, we add data on alliance start and end dates as well as geographic scope from the [Federal Maritime Commission Agreement Library](https://www2.fmc.gov/FMC.Agreements.Web/Public), as well as [vessel capacity data](https://ndclibrary.sec.usace.army.mil/searchResults?series=Foreign%20Traffic%20Vessel%20Entrances%20Clearances) from the US Army Corp of Engineers. \n",
    "\n",
    "See the github repo and the README for more detail. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preliminaries \n",
    "import pandas as pd #v2.1.3\n",
    "import polars as pl #v1.1.0\n",
    "import plotly_express as px #v0.4.1 \n",
    "import datetime as dt\n",
    "from sklearn.cluster import HDBSCAN\n",
    "import time\n",
    "import os\n",
    "\n",
    "\n",
    "#display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "#enable string cache for polars categoricals\n",
    "pl.enable_string_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PIERS BOL Data\n",
    "\n",
    "Our primary dataset for this study comes from the [PIERS Data Project](https://github.com/epistemetrica/PIERS-Data-Project). The initial codeblock loads the relevant columns from this database into seperate Polars LazyFrames for the import and export data, and subsequent blocks address the various issues in the data. Although the PIERS Data Project includes data from Q12005 through Q12024, we limit the data here to the beginning of 2007 through the end of 2023, as the pre-2007 has outstanding structural issues and we prefer to look at entire years. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load PIERS bol data lazyframes\n",
    "imports_lf = (\n",
    "    pl.scan_parquet('../data/piers_raw/imports/*.parquet', parallel='columns')\n",
    "    #drop unused columns \n",
    "    .select(\n",
    "        #'weight',\n",
    "        #'weight_unit',\n",
    "        #'qty',\n",
    "        #'qty_type',\n",
    "        'teus',\n",
    "        #'value_est',\n",
    "        'date',\n",
    "        #'container_piece_count',\n",
    "        #'commod_short_desc_qty',\n",
    "        'origin_territory',\n",
    "        'origin_region',\n",
    "        'arrival_port_code',\n",
    "        'arrival_port_name',\n",
    "        'departure_port_code',\n",
    "        'departure_port_name',\n",
    "        #'dest_final',\n",
    "        'coast_region',\n",
    "        #'clearing_district',\n",
    "        #'place_receipt',\n",
    "        #'shipper_name',\n",
    "        #'shipper_address',\n",
    "        #'consignee_name',\n",
    "        #'consignee_address',\n",
    "        #'notify_party1_name',\n",
    "        #'notify_party1_address',\n",
    "        #'notify_party2_name',\n",
    "        #'notify_party2_address',\n",
    "        #'commod_desc_raw',\n",
    "        #'container_id_marks',\n",
    "        #'marks_desc',\n",
    "        'hs_code',\n",
    "        #'joc_code',\n",
    "        #'commod_short_desc',\n",
    "        #'container_ids',\n",
    "        'carrier_name',\n",
    "        'carrier_scac',\n",
    "        'vessel_name',\n",
    "        'voyage_number',\n",
    "        #'precarrier',\n",
    "        'vessel_id',\n",
    "        #'inbond_code',\n",
    "        #'transport_mode',\n",
    "        #'bol_number',\n",
    "        'direction',\n",
    "        'bol_id',\n",
    "        'year',\n",
    "        'month',\n",
    "        'lane_id'\n",
    "    )\n",
    "    #get lane name \n",
    "    .with_columns(\n",
    "            #find most commonly used departure port name for a given lane_id\n",
    "            pl.col('departure_port_name').drop_nulls().mode().first().over('lane_id').alias('best_departure_port_name'),\n",
    "            #find most commonly used arrival port name for a given lane_id\n",
    "            pl.col('arrival_port_name').drop_nulls().mode().first().over('lane_id').alias('best_arrival_port_name')\n",
    "        )\n",
    "        .with_columns(\n",
    "            (pl.col('best_departure_port_name').cast(pl.Utf8)+' — '+pl.col('best_arrival_port_name').cast(pl.Utf8))\n",
    "            .str.to_titlecase()\n",
    "            .cast(pl.Categorical)\n",
    "            .alias('lane_name')\n",
    "        )\n",
    "        .drop('best_departure_port_name', 'best_arrival_port_name')\n",
    "    #filter dates\n",
    "    .filter(pl.col('year').is_in(range(2007, 2024)))\n",
    ")\n",
    "\n",
    "exports_lf = (\n",
    "    pl.scan_parquet('../data/piers_raw/exports/piers_exports_raw.parquet', parallel='columns') \n",
    "    #drop unused columns\n",
    "    .select(\n",
    "        #'shipper',\n",
    "        #'shipper_address',\n",
    "        #'weight',\n",
    "        #'weight_unit',\n",
    "        #'qty',\n",
    "        #'quantity_type',\n",
    "        'teus',\n",
    "        'carrier_name',\n",
    "        'carrier_scac',\n",
    "        'vessel_name',\n",
    "        'voyage_number',\n",
    "        #'bol_number',\n",
    "        'vessel_id',\n",
    "        #'value_est',\n",
    "        'departure_port_code',\n",
    "        'departure_port_name',\n",
    "        #'container_ids',\n",
    "        #'container_piece_count',\n",
    "        'coast_region',\n",
    "        #'commod_desc_raw',\n",
    "        #'commod_short_desc',\n",
    "        'hs_code',\n",
    "        #'joc_code',\n",
    "        #'commod_short_desc_qty',\n",
    "        'date',\n",
    "        #'origin',\n",
    "        'dest_territory',\n",
    "        'dest_region',\n",
    "        'arrival_port_code',\n",
    "        'arrival_port_name',\n",
    "        'direction',\n",
    "        'bol_id',\n",
    "        'year',\n",
    "        'month',\n",
    "        'lane_id'\n",
    "    )\n",
    "    #get lane name \n",
    "    .with_columns(\n",
    "            #find most commonly used departure port name for a given lane_id\n",
    "            pl.col('departure_port_name').drop_nulls().mode().first().over('lane_id').alias('best_departure_port_name'),\n",
    "            #find most commonly used arrival port name for a given lane_id\n",
    "            pl.col('arrival_port_name').drop_nulls().mode().first().over('lane_id').alias('best_arrival_port_name')\n",
    "        )\n",
    "        .with_columns(\n",
    "            (pl.col('best_departure_port_name').cast(pl.Utf8)+' — '+pl.col('best_arrival_port_name').cast(pl.Utf8))\n",
    "            .str.to_titlecase()\n",
    "            .cast(pl.Categorical)\n",
    "            .alias('lane_name')\n",
    "        )\n",
    "        .drop('best_departure_port_name', 'best_arrival_port_name')\n",
    "    #filter dates\n",
    "    .filter(pl.col('year').is_in(range(2007, 2024)))\n",
    ")\n",
    "\n",
    "#load alliance membership data from csv (NOTE polars parsing dates is apparently broken in 1.1.0, hence the use of pandas here)\n",
    "alliances_df = pd.read_csv('../data/misc/masterAlliances.csv')\n",
    "alliances_df.date = pd.to_datetime(alliances_df.date)\n",
    "alliances_df = (\n",
    "    pl.DataFrame(alliances_df)\n",
    "    .cast({'unified_carrier_scac':pl.Categorical, 'date':pl.Datetime})\n",
    "    .select('date', 'unified_carrier_scac', 'alliance')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create main lazyframe\n",
    "main_lf = pl.concat([imports_lf, exports_lf], how='diagonal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#notebook functions\n",
    "\n",
    "#fill nulls in volume cols with mean\n",
    "def fill_volume(lf):\n",
    "    '''ad hod function to fill volume columns with their means'''\n",
    "    return (\n",
    "        lf\n",
    "        .with_columns([\n",
    "            pl.col('teus').replace(0,None).fill_null(strategy='mean'),\n",
    "            #pl.col('weight').replace(0,None).fill_null(strategy='mean'),\n",
    "            #pl.col('qty').replace(0,None).fill_null(strategy='mean')\n",
    "            ])\n",
    "        )\n",
    "\n",
    "#plotly graph inspecting nulls over time by group\n",
    "def nulls_over_time_plotly(data_lf, group_var, time_var, value_var, title=False):\n",
    "    '''\n",
    "    Plots proportion of null values over time by group.\n",
    "    INPUTS:\n",
    "        data_lf - polars lazyframe containing the relevant data\n",
    "        group_var - str - the name of the column by which to group\n",
    "        time_var - str - the name of the time column (e.g., year, month) over which values will be counted\n",
    "        value_var - str - the name of the column containing the variable in question\n",
    "        title (default=False) - str - the title of the graph\n",
    "    OUTPUT:\n",
    "        a plotly express figure\n",
    "    DEPENDS ON:\n",
    "        polars\n",
    "        plotly express \n",
    "    '''\n",
    "    df = (\n",
    "        #select relevant columns\n",
    "        data_lf.select([group_var, time_var, value_var])\n",
    "        #group by, creating null count and non-null count cols\n",
    "        .group_by(group_var, time_var)\n",
    "        .agg([pl.col(value_var).null_count().alias('null_count'),\n",
    "                pl.col(value_var).count().alias('count')])\n",
    "        #compute percent null and fill new column\n",
    "        .with_columns((pl.col('null_count')/(pl.col('count')+pl.col('null_count'))).alias('null_percent'))\n",
    "        #cast group col to string to allow sensible ordering of legend\n",
    "        .cast({group_var:pl.Utf8})\n",
    "        #sort by date (to allow proper visualization of lines) and group (for legend ordering) \n",
    "        .sort(time_var, group_var)\n",
    "    ).collect()\n",
    "    #plot\n",
    "    fig = px.line(\n",
    "        data_frame=df,\n",
    "        x=time_var, y='null_percent',\n",
    "        color=group_var,\n",
    "        title= 'Count of nulls over time by source frame.' if not title else title\n",
    "    )\n",
    "    fig.show()\n",
    "    del df\n",
    "\n",
    "#fill nulls over groups given a single unique value per group\n",
    "def fill_nulls_by_group(data_lf, group_vars, val_var):\n",
    "    '''Fills null values by group if and only if the val_var for that group contains exactly one non-null unique value.\n",
    "    INPUTS:\n",
    "        data_lf - polars lazyframe containing the relevant data\n",
    "        group_vars - iterable - the names of the columns by which groups will be created\n",
    "        val_var - string - the name of the column in which nulls will be filled\n",
    "    OUTPUT:\n",
    "        filled_lf - the resultant lazyframe \n",
    "    DEPENDS ON:\n",
    "        polars - current version written in polars 0.20.1\n",
    "    '''\n",
    "    filled_lf = (\n",
    "        data_lf.with_columns(\n",
    "            #if the group contains exactly one unique value: \n",
    "            pl.when(pl.col(val_var).drop_nulls().unique(maintain_order=True).len().over(group_vars)==1)\n",
    "            #then fill the group with that value\n",
    "            .then(pl.col(val_var).fill_null(pl.col(val_var).drop_nulls().unique(maintain_order=True).first().over(group_vars)))\n",
    "            #otherwise do nothing\n",
    "            .otherwise(pl.col(val_var))\n",
    "            )\n",
    "        )\n",
    "    return filled_lf\n",
    "\n",
    "#assign primary carrier\n",
    "def add_primary_carrier(lf):\n",
    "    '''ad hoc function to find primary carrier for each vessel and indicate cargo sharing'''\n",
    "    lf = (\n",
    "        #sum teus over vessel, month, and carrier\n",
    "        lf.with_columns(\n",
    "            pl.col('teus').sum()\n",
    "            .over('vessel_id', 'month', 'unified_carrier_scac')\n",
    "            .alias('sum_teus')\n",
    "            )\n",
    "        #select carrier that moved the most cargo on that vessel during that month\n",
    "        .with_columns(\n",
    "            pl.col('unified_carrier_scac')\n",
    "            .sort_by('sum_teus', descending=True)\n",
    "            .drop_nulls().first()\n",
    "            .over('vessel_id', 'month')\n",
    "            .alias('vessel_owner')\n",
    "            )\n",
    "        #add bool col if bol is from primary carrier\n",
    "        .with_columns(\n",
    "            (pl.col('unified_carrier_scac')==pl.col('vessel_owner'))\n",
    "            .alias('primary_cargo')\n",
    "            )\n",
    "        #set related columns to missing when vessel_id is missing\n",
    "        .with_columns(\n",
    "            pl.when(pl.col('vessel_id').is_null()).then(pl.lit(None)).otherwise(pl.col('vessel_owner')).alias('vessel_owner'),\n",
    "            pl.when(pl.col('vessel_id').is_null()).then(pl.lit(None)).otherwise(pl.col('primary_cargo')).alias('primary_cargo')\n",
    "        )\n",
    "        #add shared teu column for convenience \n",
    "        .with_columns(\n",
    "            (pl.col('teus')*(1-pl.col('primary_cargo')))\n",
    "            .alias('shared_teus')\n",
    "        )\n",
    "        #drop ad hoc sum_teus col\n",
    "        .drop('sum_teus')\n",
    "    )\n",
    "    return lf\n",
    "\n",
    "#plot proportion of shared cargo over time\n",
    "def sharing_over_time_plotly(data_lf, group_var, include_missing_vessels=True, limit=10, title=False):\n",
    "    '''\n",
    "    Plots proportion of shared cargo over time (months) by group_var.\n",
    "    INPUTS:\n",
    "        data_lf - polars lazyframe containing the relevant data\n",
    "        group_var - str - the name of the column by which to group\n",
    "        include_missing_vessels - bool - default=True, when False, drops missing vessel_ids\n",
    "        title (default=False) - str - the title of the graph\n",
    "    OUTPUT:\n",
    "        a plotly express figure\n",
    "    DEPENDS ON:\n",
    "        polars\n",
    "        plotly express \n",
    "    '''\n",
    "    if not include_missing_vessels:\n",
    "        df = data_lf.drop_nulls('vessel_id')\n",
    "    else:\n",
    "        df = data_lf\n",
    "    \n",
    "    df = (\n",
    "        #select relevant columns\n",
    "        df.select([group_var, 'month', 'primary_cargo', 'teus'])\n",
    "        #sum teus over each group-month-shared \n",
    "        .group_by(group_var, 'month')\n",
    "        .agg(\n",
    "            (pl.col('teus')*pl.col('primary_cargo')).sum().alias('total_primary'),\n",
    "            pl.col('teus').sum().alias('total_teus')\n",
    "        )\n",
    "        #create proportion shared\n",
    "        .with_columns((1-(pl.col('total_primary')/pl.col('total_teus'))).alias('prop_shared'))\n",
    "        #cast group col to string to allow sensible ordering of legend\n",
    "        .cast({group_var:pl.Utf8})\n",
    "        #sort by date (to allow proper visualization of lines) and group (for legend ordering) \n",
    "        .sort('month')\n",
    "    ).collect()\n",
    "\n",
    "    #limit categories\n",
    "    top_groups = (\n",
    "        data_lf.group_by(group_var)\n",
    "        .agg(pl.col('teus').sum())\n",
    "        .sort('teus', descending=True)\n",
    "        .select(group_var)\n",
    "        .limit(limit)\n",
    "        .collect()\n",
    "        .to_series()\n",
    "        .cast(pl.Utf8)\n",
    "    )\n",
    "    \n",
    "    #plot\n",
    "    fig = px.line(\n",
    "        data_frame=df.filter(pl.col(group_var).is_in(top_groups)).with_columns(pl.col('month').str.to_datetime('%Y%m')),\n",
    "        x='month', y='prop_shared',\n",
    "        color=group_var,\n",
    "        title= 'Proportion of shared cargo over time.' if not title else title,\n",
    "        labels={\n",
    "            'prop_shared':'Proportion of cargo from non-primary carrier',\n",
    "            'month':'Month'\n",
    "        }\n",
    "    )\n",
    "    fig.show()\n",
    "\n",
    "def cluster_dates(lf, samples=None, save_parquet=None):\n",
    "    '''\n",
    "    Finds arrival/departure date using the following algorithm:\n",
    "        1. Create 1-D dataframe of dates for each vessel-lane pair, \n",
    "            with one date occurance per TEU processed on that date\n",
    "        2. Find clusers of dates using SciKitLearn's HDBSCAN\n",
    "        3. Assign mode date of each cluster as the arrival/departure date\n",
    "        4. Assign any bols with dates occuring between the modes as arriving/departing\n",
    "            on the date of the preceeding mode.\n",
    "        5. Join imputed arrival/departure dates into main lazyframe. \n",
    "    INPUTS\n",
    "        lf - a polars LazyFrame containing the relevant data\n",
    "        samples - int - number of random samples \n",
    "        save_parquet - path - the location where the clustering output will be saved \n",
    "    OUTPUTS\n",
    "        lf - the original lazyframe with imputed dates \n",
    "    '''\n",
    "    #create relevant columns in main lf\n",
    "    lf = (\n",
    "        lf\n",
    "        #create us_port\n",
    "        .with_columns(\n",
    "            pl.when(pl.col('direction')=='import')\n",
    "            .then(pl.col('arrival_port_code'))\n",
    "            .otherwise(pl.col('departure_port_code'))\n",
    "            .alias('us_port')\n",
    "        )\n",
    "        #create vessel_port_pair\n",
    "        .with_columns(\n",
    "            (pl.col('vessel_id').cast(pl.Utf8)+'_'+pl.col('us_port').cast(pl.Utf8))\n",
    "            .cast(pl.Categorical)\n",
    "            .alias('vessel_port_pair')\n",
    "        )\n",
    "    )\n",
    "    #collect relevant columns from lf\n",
    "    begin_collect = time.time()\n",
    "    df = (\n",
    "        lf.group_by('date', 'vessel_port_pair')\n",
    "        #get sum of TEUs on each date \n",
    "        .agg(pl.col('teus').sum().alias('sum_teus'))\n",
    "        #drop missing vessel-port pairs\n",
    "        .drop_nulls(subset=['vessel_port_pair'])\n",
    "        #sort by date\n",
    "        .sort('date')\n",
    "        .collect()\n",
    "    )\n",
    "    print('clustering data collected; time = {:.2f} minutes'.format((time.time() - begin_collect)/60))\n",
    "    #initialize variables\n",
    "    samples=samples \n",
    "    if samples:\n",
    "        pairs = df.select('vessel_port_pair').unique().sample(samples).to_series()\n",
    "    else:\n",
    "        pairs = df.select('vessel_port_pair').unique().to_series()\n",
    "    pairs_df = pl.DataFrame()\n",
    "    #loop through vessel-port pairs\n",
    "    print('Looping through vessel-port pairs')\n",
    "    for i in range(len(pairs)):\n",
    "        if i%1000 == 0:\n",
    "            begin_block = time.time()\n",
    "        pair = pairs[i]\n",
    "        #make single-column dataframe of dates where each date corresponds to a single TEU that arrived on that day \n",
    "        pair_1d = (\n",
    "            df.filter(pl.col('vessel_port_pair')==pair)\n",
    "            .select('date', pl.col('sum_teus').ceil())\n",
    "            #explode dates by each teu \n",
    "            .select(pl.exclude('sum_teus').repeat_by('sum_teus').explode())\n",
    "        )\n",
    "        #find minimum number of occurances of a single date (needed for HDBSCAN param)\n",
    "        min_sample = pair_1d.group_by('date').agg(pl.col('date').count().alias('count')).min().row(0)[1]\n",
    "        #skip empty pairs\n",
    "        if min_sample == 0:\n",
    "            continue\n",
    "        #skip vessel_port pairs with less than 2 dates\n",
    "        if len(pair_1d) < 2:\n",
    "            continue\n",
    "        #instantiate clusterer\n",
    "        clusterer = HDBSCAN(min_cluster_size=50, min_samples=min_sample) #we need to find a dynamic way of seleting these parameters\n",
    "        #get clusters\n",
    "        clusterer.fit(pair_1d)\n",
    "        #add back to pair_1d\n",
    "        pair_df = (\n",
    "            pair_1d\n",
    "            #add cluster column\n",
    "            .with_columns(\n",
    "                pl.Series(name='cluster', values=clusterer.labels_)\n",
    "            )\n",
    "            #add imputed date column\n",
    "            .with_columns(\n",
    "                    #when date matches the mode of each cluster\n",
    "                    pl.when(pl.col('date') == pl.col('date').mode().first().over('cluster'))\n",
    "                    #fill with that date, otherwise fill with null\n",
    "                    .then(pl.col('date'))\n",
    "                    .otherwise(pl.lit(None))\n",
    "                    #forward fill the arrival date to the mode of next cluster\n",
    "                    .forward_fill()\n",
    "                    #backward fill the first part of first cluster\n",
    "                    .backward_fill()\n",
    "                    #name column\n",
    "                    .alias('date_imputed')\n",
    "                )\n",
    "            #groupby date to simplify\n",
    "            .group_by('date')\n",
    "            .agg(pl.col('date_imputed').first())\n",
    "            #add pair label\n",
    "            .with_columns(pl.lit(pair).alias('vessel_port_pair').cast(pl.Categorical))\n",
    "        )\n",
    "        #init or concat pairs_df\n",
    "        if i == 0:\n",
    "            pairs_df = pair_df   \n",
    "        else:\n",
    "            pairs_df = pl.concat([pairs_df,pair_df], how='vertical')\n",
    "        #print status update\n",
    "        if (i != 0) and ((i+1)%1000 == 0):\n",
    "            print('{:,} pairs of {:,} clustered. The previous 1000 pairs took {:.2f} minutes.'.format(i+1, len(pairs), (time.time()-begin_block)/60))\n",
    "    #join imputed dates to main lf\n",
    "    lf = (\n",
    "        lf.join(pairs_df.lazy(), on=['date', 'vessel_port_pair'], how='left')\n",
    "    )\n",
    "    print('Total time to cluster dates: {:.2f} hours'.format((time.time()-begin_collect)/3600))\n",
    "    #rename date columns\n",
    "    lf = lf.rename({'date':'date_raw', 'date_imputed':'date'})\n",
    "    pairs_df = pairs_df.rename({'date':'date_raw', 'date_imputed':'date'})\n",
    "    #save pairs data to parquet if indicated \n",
    "    if save_parquet:\n",
    "        pairs_df.write_parquet(save_parquet)\n",
    "    #return main lf\n",
    "    return lf\n",
    "\n",
    "def add_alliance_data(lf):\n",
    "    '''ad hoc function to add alliance info into main lazyframe'''\n",
    "    lf = (\n",
    "        lf\n",
    "         #join alliance data into main lf\n",
    "        .join(alliances_df.lazy(), on=['unified_carrier_scac', 'date'], how='left')\n",
    "        #create related columns\n",
    "        .with_columns(\n",
    "            #create boolean for alliance membership\n",
    "            pl.col('alliance').is_not_null().alias('alliance_member'),\n",
    "            #set missing alliance_member cells to \"Non-alliance Carriers\"\n",
    "            pl.col('alliance').replace({None:'Non-alliance Carriers'})\n",
    "        )\n",
    "        #get primary carrier alliance\n",
    "        .with_columns(\n",
    "            pl.when(pl.col('unified_carrier_scac')==pl.col('vessel_owner'))\n",
    "            .then(pl.col('alliance'))\n",
    "            .otherwise(pl.lit(None))\n",
    "            .alias('pc_alliance')\n",
    "        )\n",
    "        #fill nulls in primary carrier alliance over month and vessel\n",
    "        .with_columns(\n",
    "            pl.col('pc_alliance')\n",
    "            .fill_null(strategy='forward')\n",
    "            .over('vessel_id', 'month')\n",
    "        )\n",
    "        .with_columns(\n",
    "            pl.col('pc_alliance')\n",
    "            .fill_null(strategy='backward')\n",
    "            .over('vessel_id', 'month')\n",
    "        )\n",
    "        #get cargo source column\n",
    "        .with_columns(\n",
    "            pl.when((pl.col('primary_cargo')==True)&(pl.col('alliance_member')==True))\n",
    "            .then(pl.lit('ally'))\n",
    "            .otherwise(\n",
    "                pl.when((pl.col('alliance_member')==True)&(pl.col('alliance')==pl.col('pc_alliance')))\n",
    "                .then(pl.lit('ally'))\n",
    "                .otherwise(pl.lit('non-ally'))\n",
    "            )\n",
    "            .alias('cargo_source')\n",
    "        )\n",
    "    )\n",
    "    return lf \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop Duplicate BOLs\n",
    "\n",
    "Conversations with S&P indicate that BOLs are unique, thus as an initial preparation step, we drop duplicate BOLs from the database.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_lf = main_lf.unique(subset='bol_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carrier names and Standard Carrier Alpha Codes (SCAC)\n",
    "\n",
    "Carrier names are often long strings of inconsistent nature (e.g. \"Maersk\", \"MAERSK LINE\", \"A.P. Moller Maersk\", etc.), and SCAC codes can change over time for the same carrier. To address these issues, we simply carrier names to the most commonly used name string for a given SCAC, and we simplify SCAC codes to the most recent SCAC used for a given carrier name. \n",
    "\n",
    "As carrier alliances apply only to containerized freight, we also drop instances of bulk cargo, which are coded in this data as \"BULK\" under the carrier_scac column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop bulk carriers\n",
    "main_lf = main_lf.filter(pl.col('carrier_scac')!='BULK')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unify carrier names and scacs\n",
    "\n",
    "main_lf = (\n",
    "    main_lf\n",
    "    #get most commonly used form of carrier name\n",
    "    .with_columns(\n",
    "        pl.col('carrier_name').drop_nulls().mode().first().over('carrier_scac')\n",
    "        .alias('unified_carrier_name')\n",
    "    )\n",
    "    #get most commonly used form of SCAC\n",
    "    .with_columns(\n",
    "        pl.col('carrier_scac').drop_nulls().mode().first().over('unified_carrier_name')\n",
    "        .alias('unified_carrier_scac')\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tanker Transfer Ports\n",
    "\n",
    "Some BOLs list offshore tanker transfer ports as their origin or destination. Since these are not relevant to containerized carrier alliances, we drop them from the database. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_lf = (\n",
    "    main_lf\n",
    "    .filter((pl.col('departure_port_code').cast(pl.Utf8).str.starts_with('999') == False))\n",
    "    .filter(pl.col('arrival_port_code').cast(pl.Utf8).str.starts_with('999') == False)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nulls_over_time_plotly(main_lf, group_var='direction', time_var='month', value_var='teus', title='Missing volume data over time.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen above, the PIERS BOL data on TEUs is incomplete prior to 2015. Since each row corresponds to a unique bill of lading, and since we are not concerned with changes in volume at the BOL level, we fill this missing data with the average TEUs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fill missing volumes with the mean value\n",
    "main_lf = fill_volume(main_lf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Missing Vessel info\n",
    "\n",
    "A substantial portion of BOLs do not include vessel names or IDs. Note there is perfect correlation between missing vessels names and missing vessel IDs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nulls_over_time_plotly(\n",
    "    data_lf=main_lf,\n",
    "    group_var='direction',\n",
    "    time_var='month',\n",
    "    value_var='vessel_name',\n",
    "    title='Proportion of Missing Vessel Names over time.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our analysis concerns the practice of carriers sharing cargo with other carriers on a single vessel, we drop missing vessels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop missing vessels\n",
    "main_lf = main_lf.drop_nulls(subset='vessel_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also drop bols with missing port data for the same reason. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop missing ports\n",
    "main_lf = main_lf.drop_nulls(subset=['arrival_port_code', 'departure_port_code'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Establish Primary Carrier\n",
    "\n",
    "We define the primary carrier of a vessel as the carrier representing the mode of cargo on that vessel during any given month. Cargoes from the primary carrier are deemed \"primary cargo\" and cargo from other carriers are deemed \"shared cargo.\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add primary carrier\n",
    "main_lf = add_primary_carrier(main_lf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Port Visit Identification\n",
    "\n",
    "Unfortunately, the date listed on each BOL does not necessarily correspond to the actual date of the ship's US port visit, with date data spread out over several days to a week for a single vessel and port, even when the associated routes take several weeks to complete. We suspect this is due to US Customs processing delays and inconsistency in whether the PIERS data records the actual port visit date or the date on which Customs processed the BOL (i.e., the date data are noisy). We address this problem by emplying a Hierarchical Density-Based Spatial Clustering of Applications with Noise (HDBSCAN) algorithm to cluster the dates surrounding port visits together into a single date. This allows us to analyse the data with unique combinations of vessel, port, and date, as is the case for the actual activity of ships. \n",
    "\n",
    "The problem of noise in the original date data is most problematic when routes are very short (e.g., between Florida and the Dominican Republic), which is a very small percentage of cargo volumes in the database. In other words, the HDBSCAN clustering successfully solves the problem for the majority of volumes represented in the data. If more accurate date data is required, future analyses may add route distances from the SeaRoute database as a parameter in the clustering algorithm. In recent years, [vessel entrances and clearances data](https://ndclibrary.sec.usace.army.mil/searchResults?series=Foreign%20Traffic%20Vessel%20Entrances%20Clearances) are available from the US Army Corp of Engineers, which could also be used if the need arises. \n",
    "\n",
    "### HDBSCAN Clustering \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%script echo skipping\n",
    "\n",
    "#cluser imports\n",
    "main_lf = cluster_dates(\n",
    "    main_lf, \n",
    "    save_parquet='../data/misc/imputed_date_pairings.parquet'\n",
    "    );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add other data\n",
    "\n",
    "### Alliance Membership\n",
    "\n",
    "Data on which carriers are part of which alliances was collected from alliance agreements filed with the Federal Maritime Commission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_lf = add_alliance_data(main_lf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Drewery Rate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load lane matching (from lane_matching.ipynb)\n",
    "#NOTE PIERS and Drewery lanes are matched on haversine distance between ports using bing maps API\n",
    "lane_match_df = pl.read_csv('../data/rates/lane_matching.csv').select('lane_id', 'route')\n",
    "\n",
    "#load drewery data\n",
    "drewery_df = (\n",
    "    #load CSV\n",
    "    pl.read_csv('../data/rates/tidy_rates.csv')\n",
    "    #filter by US ports\n",
    "    .filter(pl.col('route').str.contains(' US '))\n",
    "    #choose cols\n",
    "    .select('route', 'container_type', 'date', 'rate')\n",
    "    #drop duplicates on relevant cols\n",
    "    .unique(subset=['route', 'container_type', 'date'])\n",
    "    #pivot container type\n",
    "    .pivot('container_type', values='rate')\n",
    "    #rename\n",
    "    .rename({\n",
    "        '40ft Dry':'rate_40',\n",
    "        '20ft Dry':'rate_20'\n",
    "    })\n",
    "    #convert date to dt\n",
    "    .with_columns(\n",
    "        pl.col('date').str.to_datetime(format='%Y-%m')\n",
    "    )\n",
    "    #cast routes to cat\n",
    "    .cast({'route':pl.Categorical})\n",
    ")\n",
    "\n",
    "#join drewery routes into main_lf\n",
    "main_lf = (\n",
    "    main_lf.join(lane_match_df.lazy().cast({'lane_id':pl.Categorical}), on='lane_id', how='left')\n",
    "    #cast route to cat\n",
    "    .cast({'route':pl.Categorical})\n",
    ")\n",
    "#create df with drewery route, piers bol date, and rate data\n",
    "lf = (\n",
    "    #get drewery route and piers bol dat\n",
    "    main_lf.select('route', 'date')\n",
    "    #sort on date\n",
    "    .sort(by='date')\n",
    "    #join rates\n",
    "    .join_asof( #join_asof matches nearest date from drewery_df with the piers bol date\n",
    "        drewery_df.lazy()\n",
    "        .sort(by='date'),\n",
    "        by='route',\n",
    "        on='date',\n",
    "        strategy='backward'\n",
    "    )\n",
    "    #drop missing dates (NOTE: there are no instances of rate_40 existing when rate_20 is missing, but the converse is not true)\n",
    "    .drop_nulls(subset=['route', 'date', 'rate_20']) \n",
    "    #drop duplicates\n",
    "    .unique()\n",
    ")\n",
    "\n",
    "#merge into main lf\n",
    "main_lf = main_lf.join(lf, on=['route', 'date'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nulls_over_time_plotly(\n",
    "    data_lf=main_lf,\n",
    "    group_var='direction',\n",
    "    time_var='month',\n",
    "    value_var='rate_20',\n",
    "    title='Proportion of Missing Rates over time.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vessel Capacities\n",
    "\n",
    "Vessel Capacity info comes from the US Army Corp of Engineers in Net Register Tonnage (NRT), which is equal to 100 cubic feet. A standard Twenty-foot Equivalent Unit (TEU) container has an external volume of 1360 cubic feet, thus we divide the NRT capacity by 13.6 in order to obtain the TEU capacity of each vessel in the database. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in vessel data\n",
    "path = '../data/vessel_e&c/'\n",
    "schema = {'IMO_UPD':pl.Int32, 'NRT':pl.Int32}\n",
    "vessels_df = pl.DataFrame(schema=schema)\n",
    "\n",
    "for file in os.listdir(path):\n",
    "    if file.endswith('.xlsx'):\n",
    "        #load data\n",
    "        file_df = pl.read_excel(path+file, columns=['IMO_UPD', 'NRT'], schema_overrides=schema)\n",
    "        #concat to main df\n",
    "        vessels_df = pl.concat([vessels_df, file_df])\n",
    "\n",
    "#polish df\n",
    "vessels_df = (\n",
    "    #match main_lf column names\n",
    "    vessels_df.rename({'IMO_UPD':'vessel_id'})\n",
    "    #drop missing values\n",
    "    .drop_nulls()\n",
    "    #drop duplicates\n",
    "    .unique()\n",
    "    #convert NRT to TEU capacity\n",
    "    .with_columns(\n",
    "        (pl.col('NRT')/13.6)\n",
    "        .alias('vessel_capacity')\n",
    "    )\n",
    "    #drop nrt\n",
    "    .drop('NRT')\n",
    ")\n",
    "\n",
    "#merge vessel capacity info to main_lf\n",
    "main_lf = main_lf.join(vessels_df.lazy(), on='vessel_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nulls_over_time_plotly(main_lf, 'direction', 'month', 'vessel_capacity')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen above, vessel capacity data is mostly comprehensive during the years available from US Corp of Engineers (2013-2022) and progressively more missing as we move outside that window."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write to Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%script echo skipping \n",
    "#write to clean parquet\n",
    "\n",
    "#get years\n",
    "years = pl.arange(2007,2024, eager=True)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for year in years:\n",
    "    print('Collecting {} dataframe...'.format(year))\n",
    "    df = (\n",
    "        main_lf\n",
    "        .filter(pl.col('year')==year)\n",
    "        .collect()\n",
    "    )\n",
    "    print('Writing {} data to parquet...'.format(year))\n",
    "    df.write_parquet(file='../data/main/main_'+str(year)+'.parquet')\n",
    "print('OCA data written to parquet')\n",
    "runtime = time.time() - start\n",
    "print('Total time to write data: {:.2f} hours'.format(runtime/3600))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wsu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
