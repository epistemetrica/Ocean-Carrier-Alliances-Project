{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation for Ocean Carrier Alliances Project\n",
    "\n",
    "This notebook loads, cleans, and prepares the data used to analyse containerized maritime freight carrier alliances. The primary data comes from S&P's PIERS BOL database, which is processed via the seperate PIERS Data Project. \n",
    "\n",
    "See the github repo and the README for more detail. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preliminaries \n",
    "import pandas as pd #v2.1.3\n",
    "import polars as pl #v0.20.18\n",
    "import plotly_express as px #v0.4.1 \n",
    "import datetime as dt\n",
    "from sklearn.cluster import HDBSCAN\n",
    "import time\n",
    "\n",
    "\n",
    "#display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "#enable string cache for polars categoricals\n",
    "pl.enable_string_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PIERS BOL Data\n",
    "\n",
    "This project uses data from the PIERS Data Project's bill of lading database. The initial codeblock loads the relevant columns from this database into seperate Polars LazyFrames for the import and export data, and subsequent blocks address the various issues in the data. \n",
    "\n",
    "Note: Since the first phase of our project is focused on PNW producers, we limit the data to only west coast ports. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load PIERS bol data lazyframes\n",
    "imports_lf = (\n",
    "    pl.scan_parquet('data/piers_raw/imports/*.parquet', parallel='columns')\n",
    "    #drop unused columns \n",
    "    .select(\n",
    "        #'weight',\n",
    "        #'weight_unit',\n",
    "        #'qty',\n",
    "        #'qty_type',\n",
    "        'teus',\n",
    "        #'value_est',\n",
    "        'date',\n",
    "        #'container_piece_count',\n",
    "        #'commod_short_desc_qty',\n",
    "        'origin_territory',\n",
    "        'origin_region',\n",
    "        'arrival_port_code',\n",
    "        'arrival_port_name',\n",
    "        'departure_port_code',\n",
    "        'departure_port_name',\n",
    "        #'dest_final',\n",
    "        'coast_region',\n",
    "        #'clearing_district',\n",
    "        #'place_receipt',\n",
    "        #'shipper_name',\n",
    "        #'shipper_address',\n",
    "        #'consignee_name',\n",
    "        #'consignee_address',\n",
    "        #'notify_party1_name',\n",
    "        #'notify_party1_address',\n",
    "        #'notify_party2_name',\n",
    "        #'notify_party2_address',\n",
    "        #'commod_desc_raw',\n",
    "        #'container_id_marks',\n",
    "        #'marks_desc',\n",
    "        'hs_code',\n",
    "        #'joc_code',\n",
    "        #'commod_short_desc',\n",
    "        #'container_ids',\n",
    "        'carrier_name',\n",
    "        'carrier_scac',\n",
    "        'vessel_name',\n",
    "        'voyage_number',\n",
    "        #'precarrier',\n",
    "        'vessel_id',\n",
    "        #'inbond_code',\n",
    "        #'transport_mode',\n",
    "        #'bol_number',\n",
    "        'direction',\n",
    "        'bol_id',\n",
    "        'year',\n",
    "        'month',\n",
    "        'lane_id'\n",
    "    )\n",
    "    #filter for west coast\n",
    "    #.filter(pl.col('coast_region')=='WEST')\n",
    "    #get lane name \n",
    "    .with_columns(\n",
    "            #find most commonly used departure port name for a given lane_id\n",
    "            pl.col('departure_port_name').drop_nulls().mode().first().over('lane_id').alias('best_departure_port_name'),\n",
    "            #find most commonly used arrival port name for a given lane_id\n",
    "            pl.col('arrival_port_name').drop_nulls().mode().first().over('lane_id').alias('best_arrival_port_name')\n",
    "        )\n",
    "        .with_columns(\n",
    "            (pl.col('best_departure_port_name').cast(pl.Utf8)+' — '+pl.col('best_arrival_port_name').cast(pl.Utf8))\n",
    "            .str.to_titlecase()\n",
    "            .cast(pl.Categorical)\n",
    "            .alias('lane_name')\n",
    "        )\n",
    "        .drop('best_departure_port_name', 'best_arrival_port_name')\n",
    ")\n",
    "\n",
    "exports_lf = (\n",
    "    pl.scan_parquet('data/piers_raw/exports/piers_exports_raw.parquet', parallel='columns') \n",
    "    #drop unused columns\n",
    "    .select(\n",
    "        #'shipper',\n",
    "        #'shipper_address',\n",
    "        #'weight',\n",
    "        #'weight_unit',\n",
    "        #'qty',\n",
    "        #'quantity_type',\n",
    "        'teus',\n",
    "        'carrier_name',\n",
    "        'carrier_scac',\n",
    "        'vessel_name',\n",
    "        'voyage_number',\n",
    "        #'bol_number',\n",
    "        'vessel_id',\n",
    "        #'value_est',\n",
    "        'departure_port_code',\n",
    "        'departure_port_name',\n",
    "        #'container_ids',\n",
    "        #'container_piece_count',\n",
    "        'coast_region',\n",
    "        #'commod_desc_raw',\n",
    "        #'commod_short_desc',\n",
    "        'hs_code',\n",
    "        #'joc_code',\n",
    "        #'commod_short_desc_qty',\n",
    "        'date',\n",
    "        #'origin',\n",
    "        'dest_territory',\n",
    "        'dest_region',\n",
    "        'arrival_port_code',\n",
    "        'arrival_port_name',\n",
    "        'direction',\n",
    "        'bol_id',\n",
    "        'year',\n",
    "        'month',\n",
    "        'lane_id'\n",
    "    )\n",
    "    #filter for west coast\n",
    "    #.filter(pl.col('coast_region')=='WEST')\n",
    "    #get lane name \n",
    "    .with_columns(\n",
    "            #find most commonly used departure port name for a given lane_id\n",
    "            pl.col('departure_port_name').drop_nulls().mode().first().over('lane_id').alias('best_departure_port_name'),\n",
    "            #find most commonly used arrival port name for a given lane_id\n",
    "            pl.col('arrival_port_name').drop_nulls().mode().first().over('lane_id').alias('best_arrival_port_name')\n",
    "        )\n",
    "        .with_columns(\n",
    "            (pl.col('best_departure_port_name').cast(pl.Utf8)+' — '+pl.col('best_arrival_port_name').cast(pl.Utf8))\n",
    "            .str.to_titlecase()\n",
    "            .cast(pl.Categorical)\n",
    "            .alias('lane_name')\n",
    "        )\n",
    "        .drop('best_departure_port_name', 'best_arrival_port_name')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#project functions\n",
    "\n",
    "#fill nulls in volume cols with mean\n",
    "def fill_volume(lf):\n",
    "    '''ad hod function to fill volume columns with their means'''\n",
    "    return (\n",
    "        lf\n",
    "        .with_columns([\n",
    "            pl.col('teus').replace(0,None).fill_null(strategy='mean'),\n",
    "            #pl.col('weight').replace(0,None).fill_null(strategy='mean'),\n",
    "            #pl.col('qty').replace(0,None).fill_null(strategy='mean')\n",
    "            ])\n",
    "        )\n",
    "\n",
    "#plotly graph inspecting nulls over time by group\n",
    "def nulls_over_time_plotly(data_lf, group_var, time_var, value_var, title=False):\n",
    "    '''\n",
    "    Plots proportion of null values over time by group.\n",
    "    INPUTS:\n",
    "        data_lf - polars lazyframe containing the relevant data\n",
    "        group_var - str - the name of the column by which to group\n",
    "        time_var - str - the name of the time column (e.g., year, month) over which values will be counted\n",
    "        value_var - str - the name of the column containing the variable in question\n",
    "        title (default=False) - str - the title of the graph\n",
    "    OUTPUT:\n",
    "        a plotly express figure\n",
    "    DEPENDS ON:\n",
    "        polars\n",
    "        plotly express \n",
    "    '''\n",
    "    df = (\n",
    "        #select relevant columns\n",
    "        data_lf.select([group_var, time_var, value_var])\n",
    "        #group by, creating null count and non-null count cols\n",
    "        .group_by(group_var, time_var)\n",
    "        .agg([pl.col(value_var).null_count().alias('null_count'),\n",
    "                pl.col(value_var).count().alias('count')])\n",
    "        #compute percent null and fill new column\n",
    "        .with_columns((pl.col('null_count')/(pl.col('count')+pl.col('null_count'))).alias('null_percent'))\n",
    "        #cast group col to string to allow sensible ordering of legend\n",
    "        .cast({group_var:pl.Utf8})\n",
    "        #sort by date (to allow proper visualization of lines) and group (for legend ordering) \n",
    "        .sort(time_var, group_var)\n",
    "    ).collect()\n",
    "    #plot\n",
    "    fig = px.line(\n",
    "        data_frame=df,\n",
    "        x=time_var, y='null_percent',\n",
    "        color=group_var,\n",
    "        title= 'Count of nulls over time by source frame.' if not title else title\n",
    "    )\n",
    "    fig.show()\n",
    "    del df\n",
    "\n",
    "#fill nulls over groups given a single unique value per group\n",
    "def fill_nulls_by_group(data_lf, group_vars, val_var):\n",
    "    '''Fills null values by group if and only if the val_var for that group contains exactly one non-null unique value.\n",
    "    INPUTS:\n",
    "        data_lf - polars lazyframe containing the relevant data\n",
    "        group_vars - iterable - the names of the columns by which groups will be created\n",
    "        val_var - string - the name of the column in which nulls will be filled\n",
    "    OUTPUT:\n",
    "        filled_lf - the resultant lazyframe \n",
    "    DEPENDS ON:\n",
    "        polars - current version written in polars 0.20.1\n",
    "    '''\n",
    "    filled_lf = (\n",
    "        data_lf.with_columns(\n",
    "            #if the group contains exactly one unique value: \n",
    "            pl.when(pl.col(val_var).drop_nulls().unique(maintain_order=True).len().over(group_vars)==1)\n",
    "            #then fill the group with that value\n",
    "            .then(pl.col(val_var).fill_null(pl.col(val_var).drop_nulls().unique(maintain_order=True).first().over(group_vars)))\n",
    "            #otherwise do nothing\n",
    "            .otherwise(pl.col(val_var))\n",
    "            )\n",
    "        )\n",
    "    return filled_lf\n",
    "\n",
    "#assign primary carrier\n",
    "def add_primary_carrier(lf):\n",
    "    '''ad hoc function to find primary carrier for each vessel and indicate cargo sharing'''\n",
    "    lf = (\n",
    "        #sum teus over vessel, month, and carrier\n",
    "        lf.with_columns(\n",
    "            pl.col('teus').sum()\n",
    "            .over('vessel_id', 'month', 'unified_carrier_scac')\n",
    "            .alias('sum_teus')\n",
    "            )\n",
    "        #select carrier that moved the most cargo on that vessel during that month\n",
    "        .with_columns(\n",
    "            pl.col('unified_carrier_scac')\n",
    "            .sort_by('sum_teus', descending=True)\n",
    "            .drop_nulls().first()\n",
    "            .over('vessel_id', 'month')\n",
    "            .alias('vessel_owner')\n",
    "            )\n",
    "        #add bool col if bol is from primary carrier\n",
    "        .with_columns(\n",
    "            (pl.col('unified_carrier_scac')==pl.col('vessel_owner'))\n",
    "            .alias('primary_cargo')\n",
    "            )\n",
    "        #set related columns to missing when vessel_id is missing\n",
    "        .with_columns(\n",
    "            pl.when(pl.col('vessel_id').is_null()).then(pl.lit(None)).otherwise(pl.col('vessel_owner')).alias('vessel_owner'),\n",
    "            pl.when(pl.col('vessel_id').is_null()).then(pl.lit(None)).otherwise(pl.col('primary_cargo')).alias('primary_cargo')\n",
    "        )\n",
    "        #drop ad hoc sum_teus col\n",
    "        .drop('sum_teus')\n",
    "    )\n",
    "    return lf\n",
    "\n",
    "#plot proportion of shared cargo over time\n",
    "def sharing_over_time_plotly(data_lf, group_var, include_missing_vessels=True, limit=10, title=False):\n",
    "    '''\n",
    "    Plots proportion of shared cargo over time (months) by group_var.\n",
    "    INPUTS:\n",
    "        data_lf - polars lazyframe containing the relevant data\n",
    "        group_var - str - the name of the column by which to group\n",
    "        include_missing_vessels - bool - default=True, when False, drops missing vessel_ids\n",
    "        title (default=False) - str - the title of the graph\n",
    "    OUTPUT:\n",
    "        a plotly express figure\n",
    "    DEPENDS ON:\n",
    "        polars\n",
    "        plotly express \n",
    "    '''\n",
    "    if not include_missing_vessels:\n",
    "        df = data_lf.drop_nulls('vessel_id')\n",
    "    else:\n",
    "        df = data_lf\n",
    "    \n",
    "    df = (\n",
    "        #select relevant columns\n",
    "        df.select([group_var, 'month', 'primary_cargo', 'teus'])\n",
    "        #sum teus over each group-month-shared \n",
    "        .group_by(group_var, 'month')\n",
    "        .agg(\n",
    "            (pl.col('teus')*pl.col('primary_cargo')).sum().alias('total_primary'),\n",
    "            pl.col('teus').sum().alias('total_teus')\n",
    "        )\n",
    "        #create proportion shared\n",
    "        .with_columns((1-(pl.col('total_primary')/pl.col('total_teus'))).alias('prop_shared'))\n",
    "        #cast group col to string to allow sensible ordering of legend\n",
    "        .cast({group_var:pl.Utf8})\n",
    "        #sort by date (to allow proper visualization of lines) and group (for legend ordering) \n",
    "        .sort('month')\n",
    "    ).collect()\n",
    "\n",
    "    #limit categories\n",
    "    top_groups = (\n",
    "        data_lf.group_by(group_var)\n",
    "        .agg(pl.col('teus').sum())\n",
    "        .sort('teus', descending=True)\n",
    "        .select(group_var)\n",
    "        .limit(limit)\n",
    "        .collect()\n",
    "        .to_series()\n",
    "        .cast(pl.Utf8)\n",
    "    )\n",
    "    \n",
    "    #plot\n",
    "    fig = px.line(\n",
    "        data_frame=df.filter(pl.col(group_var).is_in(top_groups)).with_columns(pl.col('month').str.to_datetime('%Y%m')),\n",
    "        x='month', y='prop_shared',\n",
    "        color=group_var,\n",
    "        title= 'Proportion of shared cargo over time.' if not title else title,\n",
    "        labels={\n",
    "            'prop_shared':'Proportion of cargo from non-primary carrier',\n",
    "            'month':'Month'\n",
    "        }\n",
    "    )\n",
    "    fig.show()\n",
    "\n",
    "def cluster_dates(lf, direction, samples=None):\n",
    "    '''\n",
    "    Finds arrival/departure date using the following algorithm:\n",
    "        1. Create 1-D dataframe of dates for each vessel-lane pair, \n",
    "            with one date occurance per TEU processed on that date\n",
    "        2. Find clusers of dates using SciKitLearn's HDBSCAN\n",
    "        3. Assign mode date of each cluster as the arrival/departure date\n",
    "        4. Assign any bols with dates occuring between the modes as arriving/departing\n",
    "            on the date of the preceeding mode.\n",
    "        5. Join imputed arrival/departure dates into main lazyframe. \n",
    "    INPUTS\n",
    "        lf - a polars LazyFrame containing the relevant data\n",
    "        direction - 'imports' or 'exports' - indicating the source data\n",
    "        samples - int - number of random samples \n",
    "    OUTPUTS\n",
    "        lf - the original lazyframe with imputed dates \n",
    "    '''\n",
    "    #create vessel_port_pair columns in main lf\n",
    "    lf = (\n",
    "        lf.with_columns(\n",
    "            (pl.col('vessel_id').cast(pl.Utf8)+'_'+pl.col('lane_id').cast(pl.Utf8))\n",
    "            .cast(pl.Categorical)\n",
    "            .alias('vessel_lane_pair')\n",
    "        )\n",
    "    )\n",
    "    #collect relevant columns from lf\n",
    "    begin_collect = time.time()\n",
    "    df = (\n",
    "        lf.group_by('date', 'vessel_lane_pair')\n",
    "        #get sum of TEUs on each date \n",
    "        .agg(pl.col('teus').sum().alias('sum_teus'))\n",
    "        #drop missing vessel-port pairs\n",
    "        .drop_nulls(subset='vessel_lane_pair')\n",
    "        #sort by date\n",
    "        .sort('date')\n",
    "        .collect()\n",
    "    )\n",
    "    print('clustering data collected; time = {:.2f} minutes'.format((time.time() - begin_collect)/60))\n",
    "    #initialize variables\n",
    "    samples=samples \n",
    "    if samples:\n",
    "        pairs = df.select('vessel_lane_pair').unique().sample(samples).to_series()\n",
    "    else:\n",
    "        pairs = df.select('vessel_lane_pair').unique().to_series()\n",
    "    pairs_df = pl.DataFrame()\n",
    "    #loop through vessel-port pairs\n",
    "    print('Looping through vessel-lane pairs')\n",
    "    for i in range(len(pairs)):\n",
    "        if i%1000 == 0:\n",
    "            begin_block = time.time()\n",
    "        pair = pairs[i]\n",
    "        #make single-column dataframe of dates where each date corresponds to a single TEU that arrived on that day \n",
    "        pair_1d = (\n",
    "            df.filter(pl.col('vessel_lane_pair')==pair)\n",
    "            .select('date', pl.col('sum_teus').ceil())\n",
    "            #explode dates by each teu \n",
    "            .select(pl.exclude('sum_teus').repeat_by('sum_teus').explode())\n",
    "        )\n",
    "        #find minimum number of occurances of a single date (needed for HDBSCAN param)\n",
    "        min_sample = pair_1d.group_by('date').agg(pl.col('date').count().alias('count')).min().row(0)[1]\n",
    "        #skip empty pairs\n",
    "        if min_sample == 0:\n",
    "            continue\n",
    "        #skip vessel_port pairs with less than 2 dates\n",
    "        if len(pair_1d) < 2:\n",
    "            continue\n",
    "        #instantiate clusterer\n",
    "        clusterer = HDBSCAN(min_cluster_size=50, min_samples=min_sample) #we need to find a dynamic way of seleting these parameters\n",
    "        #get clusters\n",
    "        clusterer.fit(pair_1d)\n",
    "        #add back to pair_1d\n",
    "        pair_df = (\n",
    "            pair_1d\n",
    "            #add cluster column\n",
    "            .with_columns(\n",
    "                pl.Series(name='cluster', values=clusterer.labels_)\n",
    "            )\n",
    "            #add imputed date column\n",
    "            .with_columns(\n",
    "                    #when date matches the mode of each cluster\n",
    "                    pl.when(pl.col('date') == pl.col('date').mode().first().over('cluster'))\n",
    "                    #fill with that date, otherwise fill with null\n",
    "                    .then(pl.col('date'))\n",
    "                    .otherwise(pl.lit(None))\n",
    "                    #forward fill the arrival date to the mode of next cluster\n",
    "                    .forward_fill()\n",
    "                    #backward fill the first part of first cluster\n",
    "                    .backward_fill()\n",
    "                    #name column\n",
    "                    .alias('date_imputed')\n",
    "                )\n",
    "            #groupby date to simplify\n",
    "            .group_by('date')\n",
    "            .agg(pl.col('date_imputed').first())\n",
    "            #add pair label\n",
    "            .with_columns(pl.lit(pair).alias('vessel_lane_pair').cast(pl.Categorical))\n",
    "        )\n",
    "        #init or concat pairs_df\n",
    "        if i == 0:\n",
    "            pairs_df = pair_df   \n",
    "        else:\n",
    "            pairs_df = pl.concat([pairs_df,pair_df], how='vertical')\n",
    "        #print status update\n",
    "        if (i != 0) and ((i+1)%1000 == 0):\n",
    "            print('{:,} pairs clustered. The previous 1000 pairs took {:.2f} minutes.'.format(i+1, (time.time()-begin_block)/60))\n",
    "    #rename imputed dates based on direction\n",
    "    if direction=='import':\n",
    "        pairs_df = pairs_df.rename({'date_imputed': 'date_arrival'})\n",
    "    elif direction=='export':\n",
    "        pairs_df = pairs_df.rename({'date_imputed': 'date_departure'})\n",
    "    else:\n",
    "        raise Exception('direction must equal \"import\" or \"export\"')\n",
    "    #join imputed dates to main lf\n",
    "    pairs_lf = pairs_df.lazy()\n",
    "    lf = (\n",
    "        lf.join(pairs_lf, on=['date', 'vessel_lane_pair'], how='left')\n",
    "    )\n",
    "    print('Total time to cluster dates: {:.2f} hours'.format((time.time()-begin_collect)/3600))\n",
    "    return lf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carrier names and Standard Carrier Alpha Codes (SCAC)\n",
    "\n",
    "Carrier names are often long strings of inconsistent nature (e.g. \"Maersk\", \"MAERSK LINE\", \"A.P. Moller Maersk\", etc.), and SCAC codes can change over time for the same carrier. To address these issues, we simply carrier names to the most commonly used name string for a given SCAC, and we simplify SCAC codes to the most recent SCAC used for a given carrier name. \n",
    "\n",
    "As carrier alliances apply only to containerized freight, we also drop instances of bulk cargo, which are coded in this data as SCAC = \"BULK\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean carrier names and scac codes\n",
    "\n",
    "imports_lf = (\n",
    "    imports_lf\n",
    "    #drop bulk carriers\n",
    "    .filter(pl.col('carrier_scac')!='BULK')\n",
    "    #sort by date\n",
    "    .sort('date', descending=True)\n",
    "    #get most commonly used carrier name and scac \n",
    "    .with_columns(\n",
    "        pl.col('carrier_name').drop_nulls().mode().first().over('carrier_scac')\n",
    "        .alias('unified_carrier_name')\n",
    "    )\n",
    "    .with_columns(\n",
    "        pl.col('carrier_scac').drop_nulls().first().over('unified_carrier_name')\n",
    "        .alias('unified_carrier_scac')\n",
    "    )\n",
    ")\n",
    "\n",
    "exports_lf = (\n",
    "    exports_lf\n",
    "    #drop bulk carriers\n",
    "    .filter(pl.col('carrier_scac')!='BULK')\n",
    "    #sort by date\n",
    "    .sort('date', descending=True)\n",
    "    #get most commonly used carrier name and scac \n",
    "    .with_columns(\n",
    "        pl.col('carrier_name').drop_nulls().mode().first().over('carrier_scac')\n",
    "        .alias('unified_carrier_name')\n",
    "    )\n",
    "    .with_columns(\n",
    "        pl.col('carrier_scac').drop_nulls().first().over('unified_carrier_name')\n",
    "        .alias('unified_carrier_scac')\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tanker Transfer Ports\n",
    "\n",
    "Some BOLs in the imports data list offshore tanker transfer ports as their origin. Since these are not relevant to containerized carrier alliances, we drop them from the database. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "imports_lf = imports_lf.filter(pl.col('departure_port_code').cast(pl.Utf8).str.starts_with('999') == False)\n",
    "exports_lf = exports_lf.filter(pl.col('arrival_port_code').cast(pl.Utf8).str.starts_with('999') == False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Data\n",
    "\n",
    "The PIERS BOL data contains many missing values. Some of these, e.g. TEUs, are missing systematically related to date. The issues of missing volume data is a open question to S&P, although it has been many weeks with no response from them. In the meantime, we fill missing volume data with the mean from the rest of the dataset. (Note: if no help is to come from S&P, a nearest neighbor imputation may yield better results.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lf = pl.concat(\n",
    "    [imports_lf.select('direction', 'month', 'teus'),\n",
    "    exports_lf.select('direction', 'month', 'teus')]\n",
    ")\n",
    "nulls_over_time_plotly(lf, group_var='direction', time_var='month', value_var='teus', title='Missing volume data over time.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fill missing volumes with the mean value\n",
    "imports_lf = fill_volume(imports_lf)\n",
    "exports_lf = fill_volume(exports_lf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Missing Vessel info\n",
    "\n",
    "A substantial portion of BOLs do not include vessel names or IDs. Note there is perfect correlation between missing vessels names and missing vessel IDs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lf = pl.concat(\n",
    "    [imports_lf.select('direction', 'month', 'vessel_name'),\n",
    "    exports_lf.select('direction', 'month', 'vessel_name')]\n",
    ")\n",
    "\n",
    "nulls_over_time_plotly(\n",
    "    data_lf=lf,\n",
    "    group_var='direction',\n",
    "    time_var='month',\n",
    "    value_var='vessel_name',\n",
    "    title='Proportion of Missing Vessel Names over time.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our analysis concerns the practice of carriers sharing cargo with other carriers on a single vessel, we drop missing vessels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop missing vessels\n",
    "imports_lf = imports_lf.drop_nulls(subset='vessel_id')\n",
    "exports_lf = exports_lf.drop_nulls(subset='vessel_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also drop bols with missing port data for the same reason. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop missing ports\n",
    "imports_lf = imports_lf.drop_nulls(subset=['arrival_port_code', 'departure_port_code'])\n",
    "exports_lf = exports_lf.drop_nulls(subset=['arrival_port_code', 'departure_port_code'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sharing Over Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add primary carrier\n",
    "imports_lf = add_primary_carrier(imports_lf)\n",
    "exports_lf = add_primary_carrier(exports_lf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sharing_over_time_plotly(exports_lf, group_var='departure_port_name')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Voyage Identification\n",
    "\n",
    "We define a 'voyage' as a single vessel's trip from the port of departure to the port of arrival, i.e., a single vessel on a single *lane*. Sadly, the voyage IDs listed on BOLs are not consistent with this conception of a voyage (for a given vessel, lane, and date, the \"voyage id\" is far from unique), and the date listed on each BOL does not necessarily correspond to the actual arrival date of the ship (i.e., the date data are noisy). We address this problem in two steps. \n",
    "\n",
    "First, we compute route distances between each foreign/domestic port pair using [SeaRoute](https://github.com/eurostat/searoute). Using this distance we determine an estimated minimum number of days required to sail from one port to the other. This gives us a means of differentiating between multiple dates that correspond to the same port visit on the one hand, and two seperate port visits on the other. For example, if a BOL dated July 15 shows a vessel delivering a set of containers from Hong Kong to San Francisco, and a different BOL shows the same vessel delivering a different set of containers from Hong Kong to San Francisco on July 20th, we know that the vessel could not have visited Hong Kong between those port visits. Thus, we can cluster dates corresponding to a single port visit whenever the dates are close enough to preclude a visit to the other port. \n",
    "\n",
    "Second, we employ this minimum \"turn\" (from one domestic port to a foreign port and back) time as one of the parameters of a Hierarchical Density-Based Spatial Clustering of Applications with Noise (HDBSCAN) algorithm to group port visits together into a single voyage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script echo skipping\n",
    "#load searoute data \n",
    "route_distances_df = pl.read_csv('data/route_distances.csv')\n",
    "#add relevant columns\n",
    "route_distances_df = (\n",
    "    route_distances_df.with_columns(\n",
    "        #add minimum round trip time in days, assuming speed of 25kt (46km/h)\n",
    "        ((pl.col('distKM')/46/24)*2).alias('route_min_days'),\n",
    "        #create key for joining to exports lf\n",
    "        (pl.col('us_port_name').cast(pl.Utf8)+' — '+pl.col('foreign_port_name').cast(pl.Utf8))\n",
    "        .str.to_titlecase().cast(pl.Categorical)\n",
    "        .alias('exports_lane_name'),\n",
    "        #create key for joinign to imports lf\n",
    "        (pl.col('foreign_port_name').cast(pl.Utf8)+' — '+pl.col('us_port_name').cast(pl.Utf8))\n",
    "        .str.to_titlecase().cast(pl.Categorical)\n",
    "        .alias('imports_lane_name')\n",
    "        )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the large imports dataset, merging route distances directly is beyond the scope of available resources. As a work-around, the data is merged on a year-by-year basis and saved to parquet, then re-loaded. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script echo skipping\n",
    "\n",
    "#merge route distances to imports year by year and write to parquet\n",
    "years = pl.arange(2005,2025, eager=True)\n",
    "for year in years:\n",
    "    print('Collecting {} dataframe and joining distances...'.format(year))\n",
    "    df = (\n",
    "        imports_lf\n",
    "        .filter(pl.col('year')==year)\n",
    "        .collect()\n",
    "    )\n",
    "    df = df.join(route_distances_df.select('route_min_days', 'imports_lane_name'), how='left', left_on='lane_name', right_on='imports_lane_name')\n",
    "    print('Writing {} data to parquet...'.format(year))\n",
    "    df.write_parquet(file='data/imports/imports_'+str(year)+'.parquet')\n",
    "print('Imports data written to parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script echo skipping\n",
    "#join route distances to exports lf\n",
    "exports_lf = exports_lf.join(route_distances_df.select('route_min_days', 'exports_lane_name').lazy(), \n",
    "                             how='left', left_on='lane_name', right_on='exports_lane_name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script echo skipping\n",
    "#re-load imports lazyframe from parquet\n",
    "imports_lf = pl.scan_parquet('data/imports/*.parquet')\n",
    "imports_lf.select('route_min_days').collect().describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Route minimum days column glitch\n",
    "\n",
    "The script to join the route distances/days to the imports data seems to work, but as of 4/5 attempting to load the column as above throws Panic Exception ComputeError(ErrString(\"validity mask length must match the number of values\"). This merits further investigation, but for now we will move on without utilizing this parameter in the clustering algorithm. \n",
    "\n",
    "Another note: after joining the route distances to the dataframe, the clustering algorithm quickly maxes out memory, even with a 10 bol sample and even without attempting to incorporate the distances into hdbscan. The reason for this is unknown, but clustering the data *before* joining in the route distances works as expected.  \n",
    "\n",
    "### HDBSCAN Clustering \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imports_lf = cluster_dates(imports_lf, direction='import');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clustering data collected; time = 0.19 minutes\n",
      "Looping through vessel-lane pairs\n",
      "2,000 pairs clustered. The previous 1000 pairs took 0.03 minutes.\n",
      "3,000 pairs clustered. The previous 1000 pairs took 0.06 minutes.\n",
      "4,000 pairs clustered. The previous 1000 pairs took 0.16 minutes.\n",
      "5,000 pairs clustered. The previous 1000 pairs took 0.04 minutes.\n",
      "6,000 pairs clustered. The previous 1000 pairs took 0.10 minutes.\n",
      "7,000 pairs clustered. The previous 1000 pairs took 0.05 minutes.\n",
      "8,000 pairs clustered. The previous 1000 pairs took 0.38 minutes.\n",
      "9,000 pairs clustered. The previous 1000 pairs took 0.22 minutes.\n",
      "10,000 pairs clustered. The previous 1000 pairs took 0.12 minutes.\n",
      "11,000 pairs clustered. The previous 1000 pairs took 0.04 minutes.\n",
      "12,000 pairs clustered. The previous 1000 pairs took 0.10 minutes.\n",
      "13,000 pairs clustered. The previous 1000 pairs took 0.05 minutes.\n",
      "14,000 pairs clustered. The previous 1000 pairs took 0.05 minutes.\n",
      "15,000 pairs clustered. The previous 1000 pairs took 0.05 minutes.\n",
      "16,000 pairs clustered. The previous 1000 pairs took 5.24 minutes.\n",
      "17,000 pairs clustered. The previous 1000 pairs took 0.07 minutes.\n",
      "18,000 pairs clustered. The previous 1000 pairs took 0.09 minutes.\n",
      "19,000 pairs clustered. The previous 1000 pairs took 0.17 minutes.\n",
      "20,000 pairs clustered. The previous 1000 pairs took 0.13 minutes.\n",
      "21,000 pairs clustered. The previous 1000 pairs took 0.09 minutes.\n",
      "22,000 pairs clustered. The previous 1000 pairs took 0.07 minutes.\n",
      "23,000 pairs clustered. The previous 1000 pairs took 0.10 minutes.\n",
      "24,000 pairs clustered. The previous 1000 pairs took 0.79 minutes.\n",
      "25,000 pairs clustered. The previous 1000 pairs took 0.30 minutes.\n",
      "26,000 pairs clustered. The previous 1000 pairs took 0.15 minutes.\n",
      "27,000 pairs clustered. The previous 1000 pairs took 0.10 minutes.\n",
      "28,000 pairs clustered. The previous 1000 pairs took 0.06 minutes.\n",
      "29,000 pairs clustered. The previous 1000 pairs took 0.08 minutes.\n",
      "30,000 pairs clustered. The previous 1000 pairs took 0.15 minutes.\n",
      "31,000 pairs clustered. The previous 1000 pairs took 0.17 minutes.\n",
      "32,000 pairs clustered. The previous 1000 pairs took 0.29 minutes.\n",
      "33,000 pairs clustered. The previous 1000 pairs took 0.17 minutes.\n",
      "34,000 pairs clustered. The previous 1000 pairs took 0.07 minutes.\n",
      "35,000 pairs clustered. The previous 1000 pairs took 0.09 minutes.\n",
      "36,000 pairs clustered. The previous 1000 pairs took 0.08 minutes.\n",
      "37,000 pairs clustered. The previous 1000 pairs took 0.12 minutes.\n",
      "38,000 pairs clustered. The previous 1000 pairs took 0.05 minutes.\n",
      "39,000 pairs clustered. The previous 1000 pairs took 0.10 minutes.\n",
      "40,000 pairs clustered. The previous 1000 pairs took 0.23 minutes.\n",
      "41,000 pairs clustered. The previous 1000 pairs took 0.07 minutes.\n",
      "42,000 pairs clustered. The previous 1000 pairs took 0.07 minutes.\n",
      "43,000 pairs clustered. The previous 1000 pairs took 0.14 minutes.\n",
      "44,000 pairs clustered. The previous 1000 pairs took 0.13 minutes.\n",
      "45,000 pairs clustered. The previous 1000 pairs took 0.34 minutes.\n",
      "46,000 pairs clustered. The previous 1000 pairs took 0.08 minutes.\n",
      "47,000 pairs clustered. The previous 1000 pairs took 0.19 minutes.\n",
      "48,000 pairs clustered. The previous 1000 pairs took 0.07 minutes.\n",
      "49,000 pairs clustered. The previous 1000 pairs took 0.12 minutes.\n",
      "50,000 pairs clustered. The previous 1000 pairs took 0.08 minutes.\n",
      "51,000 pairs clustered. The previous 1000 pairs took 0.09 minutes.\n",
      "53,000 pairs clustered. The previous 1000 pairs took 0.06 minutes.\n",
      "54,000 pairs clustered. The previous 1000 pairs took 0.07 minutes.\n",
      "55,000 pairs clustered. The previous 1000 pairs took 0.07 minutes.\n",
      "56,000 pairs clustered. The previous 1000 pairs took 0.09 minutes.\n",
      "57,000 pairs clustered. The previous 1000 pairs took 0.09 minutes.\n",
      "58,000 pairs clustered. The previous 1000 pairs took 0.09 minutes.\n",
      "59,000 pairs clustered. The previous 1000 pairs took 0.24 minutes.\n",
      "60,000 pairs clustered. The previous 1000 pairs took 0.08 minutes.\n",
      "61,000 pairs clustered. The previous 1000 pairs took 0.29 minutes.\n",
      "62,000 pairs clustered. The previous 1000 pairs took 0.10 minutes.\n",
      "63,000 pairs clustered. The previous 1000 pairs took 0.10 minutes.\n",
      "64,000 pairs clustered. The previous 1000 pairs took 0.19 minutes.\n",
      "65,000 pairs clustered. The previous 1000 pairs took 0.09 minutes.\n",
      "66,000 pairs clustered. The previous 1000 pairs took 0.11 minutes.\n",
      "67,000 pairs clustered. The previous 1000 pairs took 0.10 minutes.\n",
      "68,000 pairs clustered. The previous 1000 pairs took 0.08 minutes.\n",
      "69,000 pairs clustered. The previous 1000 pairs took 0.09 minutes.\n",
      "70,000 pairs clustered. The previous 1000 pairs took 0.49 minutes.\n",
      "71,000 pairs clustered. The previous 1000 pairs took 0.09 minutes.\n",
      "72,000 pairs clustered. The previous 1000 pairs took 0.14 minutes.\n",
      "73,000 pairs clustered. The previous 1000 pairs took 0.12 minutes.\n",
      "74,000 pairs clustered. The previous 1000 pairs took 0.15 minutes.\n",
      "76,000 pairs clustered. The previous 1000 pairs took 0.19 minutes.\n",
      "77,000 pairs clustered. The previous 1000 pairs took 0.16 minutes.\n",
      "78,000 pairs clustered. The previous 1000 pairs took 0.10 minutes.\n",
      "79,000 pairs clustered. The previous 1000 pairs took 0.09 minutes.\n",
      "80,000 pairs clustered. The previous 1000 pairs took 0.12 minutes.\n",
      "82,000 pairs clustered. The previous 1000 pairs took 0.37 minutes.\n",
      "85,000 pairs clustered. The previous 1000 pairs took 0.09 minutes.\n",
      "86,000 pairs clustered. The previous 1000 pairs took 0.12 minutes.\n",
      "87,000 pairs clustered. The previous 1000 pairs took 0.12 minutes.\n",
      "88,000 pairs clustered. The previous 1000 pairs took 0.33 minutes.\n",
      "89,000 pairs clustered. The previous 1000 pairs took 0.10 minutes.\n",
      "90,000 pairs clustered. The previous 1000 pairs took 0.11 minutes.\n",
      "91,000 pairs clustered. The previous 1000 pairs took 0.20 minutes.\n",
      "92,000 pairs clustered. The previous 1000 pairs took 0.10 minutes.\n",
      "93,000 pairs clustered. The previous 1000 pairs took 0.10 minutes.\n",
      "94,000 pairs clustered. The previous 1000 pairs took 0.11 minutes.\n",
      "95,000 pairs clustered. The previous 1000 pairs took 0.10 minutes.\n",
      "96,000 pairs clustered. The previous 1000 pairs took 0.12 minutes.\n",
      "97,000 pairs clustered. The previous 1000 pairs took 0.14 minutes.\n",
      "98,000 pairs clustered. The previous 1000 pairs took 0.59 minutes.\n",
      "99,000 pairs clustered. The previous 1000 pairs took 0.17 minutes.\n",
      "100,000 pairs clustered. The previous 1000 pairs took 0.11 minutes.\n",
      "101,000 pairs clustered. The previous 1000 pairs took 0.11 minutes.\n",
      "102,000 pairs clustered. The previous 1000 pairs took 0.13 minutes.\n",
      "103,000 pairs clustered. The previous 1000 pairs took 0.19 minutes.\n",
      "104,000 pairs clustered. The previous 1000 pairs took 0.10 minutes.\n",
      "105,000 pairs clustered. The previous 1000 pairs took 0.12 minutes.\n",
      "106,000 pairs clustered. The previous 1000 pairs took 0.12 minutes.\n",
      "107,000 pairs clustered. The previous 1000 pairs took 0.25 minutes.\n",
      "108,000 pairs clustered. The previous 1000 pairs took 0.18 minutes.\n",
      "109,000 pairs clustered. The previous 1000 pairs took 1.03 minutes.\n",
      "110,000 pairs clustered. The previous 1000 pairs took 0.11 minutes.\n",
      "111,000 pairs clustered. The previous 1000 pairs took 0.14 minutes.\n",
      "112,000 pairs clustered. The previous 1000 pairs took 0.14 minutes.\n",
      "113,000 pairs clustered. The previous 1000 pairs took 0.12 minutes.\n",
      "114,000 pairs clustered. The previous 1000 pairs took 0.13 minutes.\n",
      "115,000 pairs clustered. The previous 1000 pairs took 0.14 minutes.\n",
      "116,000 pairs clustered. The previous 1000 pairs took 0.22 minutes.\n",
      "117,000 pairs clustered. The previous 1000 pairs took 0.11 minutes.\n",
      "118,000 pairs clustered. The previous 1000 pairs took 0.18 minutes.\n",
      "119,000 pairs clustered. The previous 1000 pairs took 10.15 minutes.\n",
      "120,000 pairs clustered. The previous 1000 pairs took 0.12 minutes.\n",
      "121,000 pairs clustered. The previous 1000 pairs took 0.13 minutes.\n",
      "122,000 pairs clustered. The previous 1000 pairs took 0.12 minutes.\n",
      "123,000 pairs clustered. The previous 1000 pairs took 0.13 minutes.\n",
      "125,000 pairs clustered. The previous 1000 pairs took 0.15 minutes.\n",
      "126,000 pairs clustered. The previous 1000 pairs took 0.13 minutes.\n",
      "127,000 pairs clustered. The previous 1000 pairs took 0.16 minutes.\n",
      "128,000 pairs clustered. The previous 1000 pairs took 0.22 minutes.\n",
      "129,000 pairs clustered. The previous 1000 pairs took 0.12 minutes.\n",
      "130,000 pairs clustered. The previous 1000 pairs took 0.13 minutes.\n",
      "131,000 pairs clustered. The previous 1000 pairs took 0.14 minutes.\n",
      "132,000 pairs clustered. The previous 1000 pairs took 0.13 minutes.\n",
      "133,000 pairs clustered. The previous 1000 pairs took 0.15 minutes.\n",
      "134,000 pairs clustered. The previous 1000 pairs took 0.13 minutes.\n",
      "135,000 pairs clustered. The previous 1000 pairs took 0.37 minutes.\n",
      "136,000 pairs clustered. The previous 1000 pairs took 0.16 minutes.\n",
      "137,000 pairs clustered. The previous 1000 pairs took 0.15 minutes.\n",
      "138,000 pairs clustered. The previous 1000 pairs took 0.19 minutes.\n",
      "139,000 pairs clustered. The previous 1000 pairs took 0.15 minutes.\n",
      "140,000 pairs clustered. The previous 1000 pairs took 0.14 minutes.\n",
      "141,000 pairs clustered. The previous 1000 pairs took 0.15 minutes.\n",
      "142,000 pairs clustered. The previous 1000 pairs took 0.13 minutes.\n",
      "143,000 pairs clustered. The previous 1000 pairs took 0.20 minutes.\n",
      "144,000 pairs clustered. The previous 1000 pairs took 0.13 minutes.\n",
      "145,000 pairs clustered. The previous 1000 pairs took 0.19 minutes.\n",
      "146,000 pairs clustered. The previous 1000 pairs took 0.18 minutes.\n",
      "147,000 pairs clustered. The previous 1000 pairs took 0.38 minutes.\n",
      "149,000 pairs clustered. The previous 1000 pairs took 0.19 minutes.\n",
      "150,000 pairs clustered. The previous 1000 pairs took 0.18 minutes.\n",
      "151,000 pairs clustered. The previous 1000 pairs took 0.15 minutes.\n",
      "152,000 pairs clustered. The previous 1000 pairs took 0.15 minutes.\n",
      "153,000 pairs clustered. The previous 1000 pairs took 0.15 minutes.\n",
      "154,000 pairs clustered. The previous 1000 pairs took 0.18 minutes.\n",
      "155,000 pairs clustered. The previous 1000 pairs took 0.16 minutes.\n",
      "156,000 pairs clustered. The previous 1000 pairs took 0.14 minutes.\n",
      "157,000 pairs clustered. The previous 1000 pairs took 0.18 minutes.\n",
      "158,000 pairs clustered. The previous 1000 pairs took 0.32 minutes.\n",
      "159,000 pairs clustered. The previous 1000 pairs took 0.19 minutes.\n",
      "160,000 pairs clustered. The previous 1000 pairs took 0.20 minutes.\n",
      "161,000 pairs clustered. The previous 1000 pairs took 0.19 minutes.\n",
      "162,000 pairs clustered. The previous 1000 pairs took 0.25 minutes.\n",
      "163,000 pairs clustered. The previous 1000 pairs took 0.19 minutes.\n",
      "164,000 pairs clustered. The previous 1000 pairs took 0.17 minutes.\n",
      "165,000 pairs clustered. The previous 1000 pairs took 0.20 minutes.\n",
      "166,000 pairs clustered. The previous 1000 pairs took 0.17 minutes.\n",
      "167,000 pairs clustered. The previous 1000 pairs took 0.17 minutes.\n",
      "168,000 pairs clustered. The previous 1000 pairs took 0.16 minutes.\n",
      "169,000 pairs clustered. The previous 1000 pairs took 0.18 minutes.\n",
      "170,000 pairs clustered. The previous 1000 pairs took 0.16 minutes.\n",
      "171,000 pairs clustered. The previous 1000 pairs took 0.42 minutes.\n",
      "172,000 pairs clustered. The previous 1000 pairs took 0.23 minutes.\n",
      "173,000 pairs clustered. The previous 1000 pairs took 0.20 minutes.\n",
      "174,000 pairs clustered. The previous 1000 pairs took 0.18 minutes.\n",
      "175,000 pairs clustered. The previous 1000 pairs took 0.18 minutes.\n",
      "176,000 pairs clustered. The previous 1000 pairs took 0.19 minutes.\n",
      "177,000 pairs clustered. The previous 1000 pairs took 0.23 minutes.\n",
      "178,000 pairs clustered. The previous 1000 pairs took 0.18 minutes.\n",
      "179,000 pairs clustered. The previous 1000 pairs took 0.18 minutes.\n",
      "180,000 pairs clustered. The previous 1000 pairs took 0.20 minutes.\n",
      "181,000 pairs clustered. The previous 1000 pairs took 0.20 minutes.\n",
      "182,000 pairs clustered. The previous 1000 pairs took 0.22 minutes.\n",
      "183,000 pairs clustered. The previous 1000 pairs took 0.28 minutes.\n",
      "184,000 pairs clustered. The previous 1000 pairs took 0.18 minutes.\n",
      "185,000 pairs clustered. The previous 1000 pairs took 0.24 minutes.\n",
      "186,000 pairs clustered. The previous 1000 pairs took 0.24 minutes.\n",
      "187,000 pairs clustered. The previous 1000 pairs took 0.21 minutes.\n",
      "188,000 pairs clustered. The previous 1000 pairs took 0.20 minutes.\n",
      "189,000 pairs clustered. The previous 1000 pairs took 0.19 minutes.\n",
      "190,000 pairs clustered. The previous 1000 pairs took 0.22 minutes.\n",
      "191,000 pairs clustered. The previous 1000 pairs took 0.18 minutes.\n",
      "192,000 pairs clustered. The previous 1000 pairs took 0.25 minutes.\n",
      "193,000 pairs clustered. The previous 1000 pairs took 0.19 minutes.\n",
      "194,000 pairs clustered. The previous 1000 pairs took 0.81 minutes.\n",
      "195,000 pairs clustered. The previous 1000 pairs took 0.26 minutes.\n",
      "196,000 pairs clustered. The previous 1000 pairs took 0.20 minutes.\n",
      "197,000 pairs clustered. The previous 1000 pairs took 0.18 minutes.\n",
      "198,000 pairs clustered. The previous 1000 pairs took 0.19 minutes.\n",
      "199,000 pairs clustered. The previous 1000 pairs took 0.19 minutes.\n",
      "200,000 pairs clustered. The previous 1000 pairs took 0.21 minutes.\n",
      "201,000 pairs clustered. The previous 1000 pairs took 0.28 minutes.\n",
      "202,000 pairs clustered. The previous 1000 pairs took 0.20 minutes.\n",
      "203,000 pairs clustered. The previous 1000 pairs took 0.21 minutes.\n",
      "204,000 pairs clustered. The previous 1000 pairs took 0.21 minutes.\n",
      "205,000 pairs clustered. The previous 1000 pairs took 0.24 minutes.\n",
      "206,000 pairs clustered. The previous 1000 pairs took 0.20 minutes.\n",
      "207,000 pairs clustered. The previous 1000 pairs took 0.22 minutes.\n",
      "208,000 pairs clustered. The previous 1000 pairs took 0.26 minutes.\n",
      "209,000 pairs clustered. The previous 1000 pairs took 0.21 minutes.\n",
      "210,000 pairs clustered. The previous 1000 pairs took 0.81 minutes.\n",
      "211,000 pairs clustered. The previous 1000 pairs took 0.22 minutes.\n",
      "212,000 pairs clustered. The previous 1000 pairs took 0.26 minutes.\n",
      "213,000 pairs clustered. The previous 1000 pairs took 0.22 minutes.\n",
      "214,000 pairs clustered. The previous 1000 pairs took 0.20 minutes.\n",
      "215,000 pairs clustered. The previous 1000 pairs took 0.21 minutes.\n",
      "216,000 pairs clustered. The previous 1000 pairs took 0.31 minutes.\n",
      "217,000 pairs clustered. The previous 1000 pairs took 0.24 minutes.\n",
      "218,000 pairs clustered. The previous 1000 pairs took 0.21 minutes.\n",
      "219,000 pairs clustered. The previous 1000 pairs took 0.34 minutes.\n",
      "220,000 pairs clustered. The previous 1000 pairs took 0.20 minutes.\n",
      "221,000 pairs clustered. The previous 1000 pairs took 0.22 minutes.\n",
      "222,000 pairs clustered. The previous 1000 pairs took 0.24 minutes.\n",
      "223,000 pairs clustered. The previous 1000 pairs took 0.21 minutes.\n",
      "224,000 pairs clustered. The previous 1000 pairs took 0.21 minutes.\n",
      "225,000 pairs clustered. The previous 1000 pairs took 0.25 minutes.\n",
      "226,000 pairs clustered. The previous 1000 pairs took 0.22 minutes.\n",
      "227,000 pairs clustered. The previous 1000 pairs took 0.21 minutes.\n",
      "228,000 pairs clustered. The previous 1000 pairs took 0.23 minutes.\n",
      "229,000 pairs clustered. The previous 1000 pairs took 0.23 minutes.\n",
      "230,000 pairs clustered. The previous 1000 pairs took 0.22 minutes.\n",
      "231,000 pairs clustered. The previous 1000 pairs took 0.45 minutes.\n",
      "232,000 pairs clustered. The previous 1000 pairs took 0.22 minutes.\n",
      "233,000 pairs clustered. The previous 1000 pairs took 0.23 minutes.\n",
      "234,000 pairs clustered. The previous 1000 pairs took 0.29 minutes.\n",
      "235,000 pairs clustered. The previous 1000 pairs took 0.22 minutes.\n",
      "236,000 pairs clustered. The previous 1000 pairs took 0.41 minutes.\n",
      "237,000 pairs clustered. The previous 1000 pairs took 0.29 minutes.\n",
      "238,000 pairs clustered. The previous 1000 pairs took 0.29 minutes.\n",
      "239,000 pairs clustered. The previous 1000 pairs took 0.38 minutes.\n",
      "240,000 pairs clustered. The previous 1000 pairs took 0.25 minutes.\n",
      "241,000 pairs clustered. The previous 1000 pairs took 0.26 minutes.\n",
      "242,000 pairs clustered. The previous 1000 pairs took 0.28 minutes.\n",
      "243,000 pairs clustered. The previous 1000 pairs took 0.39 minutes.\n",
      "244,000 pairs clustered. The previous 1000 pairs took 0.26 minutes.\n",
      "245,000 pairs clustered. The previous 1000 pairs took 0.27 minutes.\n",
      "246,000 pairs clustered. The previous 1000 pairs took 0.27 minutes.\n",
      "247,000 pairs clustered. The previous 1000 pairs took 0.27 minutes.\n",
      "248,000 pairs clustered. The previous 1000 pairs took 0.32 minutes.\n",
      "249,000 pairs clustered. The previous 1000 pairs took 0.32 minutes.\n",
      "250,000 pairs clustered. The previous 1000 pairs took 0.29 minutes.\n",
      "251,000 pairs clustered. The previous 1000 pairs took 0.26 minutes.\n",
      "252,000 pairs clustered. The previous 1000 pairs took 0.37 minutes.\n",
      "253,000 pairs clustered. The previous 1000 pairs took 0.34 minutes.\n",
      "254,000 pairs clustered. The previous 1000 pairs took 0.32 minutes.\n",
      "255,000 pairs clustered. The previous 1000 pairs took 0.33 minutes.\n",
      "256,000 pairs clustered. The previous 1000 pairs took 0.30 minutes.\n",
      "257,000 pairs clustered. The previous 1000 pairs took 0.33 minutes.\n",
      "258,000 pairs clustered. The previous 1000 pairs took 0.26 minutes.\n",
      "259,000 pairs clustered. The previous 1000 pairs took 0.27 minutes.\n",
      "260,000 pairs clustered. The previous 1000 pairs took 0.28 minutes.\n",
      "261,000 pairs clustered. The previous 1000 pairs took 0.27 minutes.\n",
      "262,000 pairs clustered. The previous 1000 pairs took 0.27 minutes.\n",
      "263,000 pairs clustered. The previous 1000 pairs took 0.37 minutes.\n",
      "264,000 pairs clustered. The previous 1000 pairs took 0.31 minutes.\n",
      "265,000 pairs clustered. The previous 1000 pairs took 0.27 minutes.\n",
      "266,000 pairs clustered. The previous 1000 pairs took 0.30 minutes.\n",
      "267,000 pairs clustered. The previous 1000 pairs took 0.45 minutes.\n",
      "268,000 pairs clustered. The previous 1000 pairs took 0.29 minutes.\n",
      "269,000 pairs clustered. The previous 1000 pairs took 0.38 minutes.\n",
      "270,000 pairs clustered. The previous 1000 pairs took 0.28 minutes.\n",
      "271,000 pairs clustered. The previous 1000 pairs took 0.46 minutes.\n",
      "272,000 pairs clustered. The previous 1000 pairs took 0.30 minutes.\n",
      "273,000 pairs clustered. The previous 1000 pairs took 0.30 minutes.\n",
      "274,000 pairs clustered. The previous 1000 pairs took 0.37 minutes.\n",
      "275,000 pairs clustered. The previous 1000 pairs took 0.31 minutes.\n",
      "276,000 pairs clustered. The previous 1000 pairs took 0.34 minutes.\n",
      "277,000 pairs clustered. The previous 1000 pairs took 0.30 minutes.\n",
      "278,000 pairs clustered. The previous 1000 pairs took 0.56 minutes.\n",
      "279,000 pairs clustered. The previous 1000 pairs took 0.66 minutes.\n",
      "280,000 pairs clustered. The previous 1000 pairs took 0.34 minutes.\n",
      "281,000 pairs clustered. The previous 1000 pairs took 0.46 minutes.\n",
      "282,000 pairs clustered. The previous 1000 pairs took 0.29 minutes.\n",
      "283,000 pairs clustered. The previous 1000 pairs took 0.30 minutes.\n",
      "284,000 pairs clustered. The previous 1000 pairs took 0.38 minutes.\n",
      "285,000 pairs clustered. The previous 1000 pairs took 0.32 minutes.\n",
      "286,000 pairs clustered. The previous 1000 pairs took 0.79 minutes.\n",
      "287,000 pairs clustered. The previous 1000 pairs took 0.46 minutes.\n",
      "288,000 pairs clustered. The previous 1000 pairs took 0.32 minutes.\n",
      "289,000 pairs clustered. The previous 1000 pairs took 0.31 minutes.\n",
      "290,000 pairs clustered. The previous 1000 pairs took 0.63 minutes.\n",
      "291,000 pairs clustered. The previous 1000 pairs took 0.33 minutes.\n",
      "292,000 pairs clustered. The previous 1000 pairs took 0.35 minutes.\n",
      "293,000 pairs clustered. The previous 1000 pairs took 0.34 minutes.\n",
      "294,000 pairs clustered. The previous 1000 pairs took 0.32 minutes.\n",
      "295,000 pairs clustered. The previous 1000 pairs took 0.60 minutes.\n",
      "297,000 pairs clustered. The previous 1000 pairs took 0.39 minutes.\n",
      "298,000 pairs clustered. The previous 1000 pairs took 0.40 minutes.\n",
      "299,000 pairs clustered. The previous 1000 pairs took 0.47 minutes.\n",
      "300,000 pairs clustered. The previous 1000 pairs took 0.32 minutes.\n",
      "301,000 pairs clustered. The previous 1000 pairs took 0.32 minutes.\n",
      "302,000 pairs clustered. The previous 1000 pairs took 1.13 minutes.\n",
      "303,000 pairs clustered. The previous 1000 pairs took 0.34 minutes.\n",
      "304,000 pairs clustered. The previous 1000 pairs took 0.34 minutes.\n",
      "305,000 pairs clustered. The previous 1000 pairs took 0.35 minutes.\n",
      "306,000 pairs clustered. The previous 1000 pairs took 0.32 minutes.\n",
      "307,000 pairs clustered. The previous 1000 pairs took 0.36 minutes.\n",
      "308,000 pairs clustered. The previous 1000 pairs took 0.37 minutes.\n",
      "309,000 pairs clustered. The previous 1000 pairs took 0.39 minutes.\n",
      "310,000 pairs clustered. The previous 1000 pairs took 0.33 minutes.\n",
      "311,000 pairs clustered. The previous 1000 pairs took 0.39 minutes.\n",
      "312,000 pairs clustered. The previous 1000 pairs took 0.37 minutes.\n",
      "313,000 pairs clustered. The previous 1000 pairs took 0.36 minutes.\n",
      "314,000 pairs clustered. The previous 1000 pairs took 0.37 minutes.\n",
      "315,000 pairs clustered. The previous 1000 pairs took 0.34 minutes.\n",
      "316,000 pairs clustered. The previous 1000 pairs took 0.38 minutes.\n",
      "317,000 pairs clustered. The previous 1000 pairs took 0.70 minutes.\n",
      "318,000 pairs clustered. The previous 1000 pairs took 0.50 minutes.\n",
      "319,000 pairs clustered. The previous 1000 pairs took 0.47 minutes.\n",
      "320,000 pairs clustered. The previous 1000 pairs took 0.44 minutes.\n",
      "321,000 pairs clustered. The previous 1000 pairs took 0.44 minutes.\n",
      "322,000 pairs clustered. The previous 1000 pairs took 0.43 minutes.\n",
      "323,000 pairs clustered. The previous 1000 pairs took 0.50 minutes.\n",
      "324,000 pairs clustered. The previous 1000 pairs took 0.44 minutes.\n",
      "325,000 pairs clustered. The previous 1000 pairs took 0.45 minutes.\n",
      "326,000 pairs clustered. The previous 1000 pairs took 0.42 minutes.\n",
      "327,000 pairs clustered. The previous 1000 pairs took 0.43 minutes.\n",
      "328,000 pairs clustered. The previous 1000 pairs took 0.49 minutes.\n",
      "329,000 pairs clustered. The previous 1000 pairs took 0.44 minutes.\n",
      "330,000 pairs clustered. The previous 1000 pairs took 0.49 minutes.\n",
      "331,000 pairs clustered. The previous 1000 pairs took 0.47 minutes.\n",
      "332,000 pairs clustered. The previous 1000 pairs took 0.46 minutes.\n",
      "333,000 pairs clustered. The previous 1000 pairs took 0.44 minutes.\n",
      "334,000 pairs clustered. The previous 1000 pairs took 0.56 minutes.\n",
      "335,000 pairs clustered. The previous 1000 pairs took 0.67 minutes.\n",
      "336,000 pairs clustered. The previous 1000 pairs took 0.46 minutes.\n",
      "337,000 pairs clustered. The previous 1000 pairs took 0.48 minutes.\n",
      "338,000 pairs clustered. The previous 1000 pairs took 0.44 minutes.\n",
      "339,000 pairs clustered. The previous 1000 pairs took 0.63 minutes.\n",
      "340,000 pairs clustered. The previous 1000 pairs took 0.45 minutes.\n",
      "341,000 pairs clustered. The previous 1000 pairs took 0.71 minutes.\n",
      "342,000 pairs clustered. The previous 1000 pairs took 0.42 minutes.\n",
      "343,000 pairs clustered. The previous 1000 pairs took 0.45 minutes.\n",
      "344,000 pairs clustered. The previous 1000 pairs took 0.58 minutes.\n",
      "345,000 pairs clustered. The previous 1000 pairs took 0.42 minutes.\n",
      "347,000 pairs clustered. The previous 1000 pairs took 0.44 minutes.\n",
      "348,000 pairs clustered. The previous 1000 pairs took 0.45 minutes.\n",
      "349,000 pairs clustered. The previous 1000 pairs took 0.44 minutes.\n",
      "350,000 pairs clustered. The previous 1000 pairs took 0.44 minutes.\n",
      "352,000 pairs clustered. The previous 1000 pairs took 0.44 minutes.\n",
      "353,000 pairs clustered. The previous 1000 pairs took 0.45 minutes.\n",
      "354,000 pairs clustered. The previous 1000 pairs took 0.47 minutes.\n",
      "355,000 pairs clustered. The previous 1000 pairs took 0.60 minutes.\n",
      "357,000 pairs clustered. The previous 1000 pairs took 0.47 minutes.\n",
      "359,000 pairs clustered. The previous 1000 pairs took 0.47 minutes.\n",
      "360,000 pairs clustered. The previous 1000 pairs took 0.58 minutes.\n",
      "361,000 pairs clustered. The previous 1000 pairs took 0.45 minutes.\n",
      "362,000 pairs clustered. The previous 1000 pairs took 0.58 minutes.\n",
      "363,000 pairs clustered. The previous 1000 pairs took 0.60 minutes.\n",
      "364,000 pairs clustered. The previous 1000 pairs took 0.46 minutes.\n",
      "365,000 pairs clustered. The previous 1000 pairs took 0.56 minutes.\n",
      "366,000 pairs clustered. The previous 1000 pairs took 0.48 minutes.\n",
      "367,000 pairs clustered. The previous 1000 pairs took 0.46 minutes.\n",
      "368,000 pairs clustered. The previous 1000 pairs took 0.52 minutes.\n",
      "369,000 pairs clustered. The previous 1000 pairs took 0.63 minutes.\n",
      "370,000 pairs clustered. The previous 1000 pairs took 0.58 minutes.\n",
      "371,000 pairs clustered. The previous 1000 pairs took 0.50 minutes.\n",
      "372,000 pairs clustered. The previous 1000 pairs took 0.90 minutes.\n",
      "373,000 pairs clustered. The previous 1000 pairs took 0.50 minutes.\n",
      "374,000 pairs clustered. The previous 1000 pairs took 0.50 minutes.\n",
      "375,000 pairs clustered. The previous 1000 pairs took 0.50 minutes.\n",
      "376,000 pairs clustered. The previous 1000 pairs took 0.51 minutes.\n",
      "377,000 pairs clustered. The previous 1000 pairs took 0.54 minutes.\n",
      "378,000 pairs clustered. The previous 1000 pairs took 0.54 minutes.\n",
      "379,000 pairs clustered. The previous 1000 pairs took 0.52 minutes.\n",
      "380,000 pairs clustered. The previous 1000 pairs took 0.54 minutes.\n",
      "381,000 pairs clustered. The previous 1000 pairs took 0.55 minutes.\n",
      "382,000 pairs clustered. The previous 1000 pairs took 0.53 minutes.\n",
      "383,000 pairs clustered. The previous 1000 pairs took 0.53 minutes.\n",
      "384,000 pairs clustered. The previous 1000 pairs took 0.68 minutes.\n",
      "385,000 pairs clustered. The previous 1000 pairs took 0.52 minutes.\n",
      "386,000 pairs clustered. The previous 1000 pairs took 0.61 minutes.\n",
      "387,000 pairs clustered. The previous 1000 pairs took 0.56 minutes.\n",
      "388,000 pairs clustered. The previous 1000 pairs took 0.56 minutes.\n",
      "389,000 pairs clustered. The previous 1000 pairs took 0.53 minutes.\n",
      "390,000 pairs clustered. The previous 1000 pairs took 0.53 minutes.\n",
      "391,000 pairs clustered. The previous 1000 pairs took 0.54 minutes.\n",
      "392,000 pairs clustered. The previous 1000 pairs took 0.63 minutes.\n",
      "393,000 pairs clustered. The previous 1000 pairs took 0.53 minutes.\n",
      "394,000 pairs clustered. The previous 1000 pairs took 0.56 minutes.\n",
      "395,000 pairs clustered. The previous 1000 pairs took 0.56 minutes.\n",
      "396,000 pairs clustered. The previous 1000 pairs took 0.62 minutes.\n",
      "397,000 pairs clustered. The previous 1000 pairs took 0.53 minutes.\n",
      "398,000 pairs clustered. The previous 1000 pairs took 0.62 minutes.\n",
      "399,000 pairs clustered. The previous 1000 pairs took 0.51 minutes.\n",
      "400,000 pairs clustered. The previous 1000 pairs took 0.53 minutes.\n",
      "401,000 pairs clustered. The previous 1000 pairs took 0.53 minutes.\n",
      "402,000 pairs clustered. The previous 1000 pairs took 0.73 minutes.\n",
      "403,000 pairs clustered. The previous 1000 pairs took 0.54 minutes.\n",
      "404,000 pairs clustered. The previous 1000 pairs took 0.65 minutes.\n",
      "405,000 pairs clustered. The previous 1000 pairs took 0.53 minutes.\n",
      "406,000 pairs clustered. The previous 1000 pairs took 0.53 minutes.\n",
      "407,000 pairs clustered. The previous 1000 pairs took 0.54 minutes.\n",
      "408,000 pairs clustered. The previous 1000 pairs took 0.85 minutes.\n",
      "409,000 pairs clustered. The previous 1000 pairs took 0.52 minutes.\n",
      "410,000 pairs clustered. The previous 1000 pairs took 0.54 minutes.\n",
      "411,000 pairs clustered. The previous 1000 pairs took 0.53 minutes.\n",
      "412,000 pairs clustered. The previous 1000 pairs took 0.55 minutes.\n",
      "413,000 pairs clustered. The previous 1000 pairs took 0.63 minutes.\n",
      "414,000 pairs clustered. The previous 1000 pairs took 1.05 minutes.\n",
      "416,000 pairs clustered. The previous 1000 pairs took 0.54 minutes.\n",
      "417,000 pairs clustered. The previous 1000 pairs took 0.55 minutes.\n",
      "418,000 pairs clustered. The previous 1000 pairs took 0.53 minutes.\n",
      "419,000 pairs clustered. The previous 1000 pairs took 0.60 minutes.\n",
      "420,000 pairs clustered. The previous 1000 pairs took 0.56 minutes.\n",
      "421,000 pairs clustered. The previous 1000 pairs took 0.53 minutes.\n",
      "422,000 pairs clustered. The previous 1000 pairs took 0.54 minutes.\n",
      "423,000 pairs clustered. The previous 1000 pairs took 0.53 minutes.\n",
      "424,000 pairs clustered. The previous 1000 pairs took 1.61 minutes.\n",
      "425,000 pairs clustered. The previous 1000 pairs took 1.98 minutes.\n",
      "426,000 pairs clustered. The previous 1000 pairs took 0.73 minutes.\n",
      "427,000 pairs clustered. The previous 1000 pairs took 0.54 minutes.\n",
      "428,000 pairs clustered. The previous 1000 pairs took 0.55 minutes.\n",
      "429,000 pairs clustered. The previous 1000 pairs took 0.54 minutes.\n",
      "430,000 pairs clustered. The previous 1000 pairs took 0.66 minutes.\n",
      "431,000 pairs clustered. The previous 1000 pairs took 0.55 minutes.\n",
      "432,000 pairs clustered. The previous 1000 pairs took 1.84 minutes.\n",
      "433,000 pairs clustered. The previous 1000 pairs took 0.55 minutes.\n",
      "434,000 pairs clustered. The previous 1000 pairs took 0.57 minutes.\n",
      "435,000 pairs clustered. The previous 1000 pairs took 0.58 minutes.\n",
      "436,000 pairs clustered. The previous 1000 pairs took 0.55 minutes.\n",
      "437,000 pairs clustered. The previous 1000 pairs took 0.56 minutes.\n",
      "438,000 pairs clustered. The previous 1000 pairs took 0.56 minutes.\n",
      "439,000 pairs clustered. The previous 1000 pairs took 0.57 minutes.\n",
      "440,000 pairs clustered. The previous 1000 pairs took 1.75 minutes.\n",
      "441,000 pairs clustered. The previous 1000 pairs took 0.53 minutes.\n",
      "443,000 pairs clustered. The previous 1000 pairs took 0.54 minutes.\n",
      "444,000 pairs clustered. The previous 1000 pairs took 0.55 minutes.\n",
      "445,000 pairs clustered. The previous 1000 pairs took 0.56 minutes.\n",
      "446,000 pairs clustered. The previous 1000 pairs took 3.39 minutes.\n",
      "447,000 pairs clustered. The previous 1000 pairs took 1.86 minutes.\n",
      "448,000 pairs clustered. The previous 1000 pairs took 0.56 minutes.\n",
      "449,000 pairs clustered. The previous 1000 pairs took 0.60 minutes.\n",
      "450,000 pairs clustered. The previous 1000 pairs took 0.92 minutes.\n",
      "451,000 pairs clustered. The previous 1000 pairs took 0.58 minutes.\n",
      "452,000 pairs clustered. The previous 1000 pairs took 0.57 minutes.\n",
      "453,000 pairs clustered. The previous 1000 pairs took 0.73 minutes.\n",
      "455,000 pairs clustered. The previous 1000 pairs took 0.74 minutes.\n",
      "456,000 pairs clustered. The previous 1000 pairs took 0.56 minutes.\n",
      "457,000 pairs clustered. The previous 1000 pairs took 0.58 minutes.\n",
      "459,000 pairs clustered. The previous 1000 pairs took 0.69 minutes.\n",
      "460,000 pairs clustered. The previous 1000 pairs took 0.58 minutes.\n",
      "461,000 pairs clustered. The previous 1000 pairs took 0.60 minutes.\n",
      "462,000 pairs clustered. The previous 1000 pairs took 0.79 minutes.\n",
      "463,000 pairs clustered. The previous 1000 pairs took 0.57 minutes.\n",
      "464,000 pairs clustered. The previous 1000 pairs took 0.64 minutes.\n",
      "465,000 pairs clustered. The previous 1000 pairs took 0.61 minutes.\n",
      "466,000 pairs clustered. The previous 1000 pairs took 0.60 minutes.\n",
      "467,000 pairs clustered. The previous 1000 pairs took 0.59 minutes.\n",
      "468,000 pairs clustered. The previous 1000 pairs took 0.59 minutes.\n",
      "469,000 pairs clustered. The previous 1000 pairs took 0.62 minutes.\n",
      "470,000 pairs clustered. The previous 1000 pairs took 0.58 minutes.\n",
      "471,000 pairs clustered. The previous 1000 pairs took 0.63 minutes.\n",
      "472,000 pairs clustered. The previous 1000 pairs took 0.65 minutes.\n",
      "473,000 pairs clustered. The previous 1000 pairs took 0.64 minutes.\n",
      "474,000 pairs clustered. The previous 1000 pairs took 0.67 minutes.\n",
      "475,000 pairs clustered. The previous 1000 pairs took 0.66 minutes.\n",
      "476,000 pairs clustered. The previous 1000 pairs took 0.71 minutes.\n",
      "477,000 pairs clustered. The previous 1000 pairs took 0.72 minutes.\n",
      "478,000 pairs clustered. The previous 1000 pairs took 0.73 minutes.\n",
      "479,000 pairs clustered. The previous 1000 pairs took 0.71 minutes.\n",
      "480,000 pairs clustered. The previous 1000 pairs took 0.75 minutes.\n",
      "481,000 pairs clustered. The previous 1000 pairs took 0.71 minutes.\n",
      "482,000 pairs clustered. The previous 1000 pairs took 0.80 minutes.\n",
      "483,000 pairs clustered. The previous 1000 pairs took 0.97 minutes.\n",
      "484,000 pairs clustered. The previous 1000 pairs took 0.70 minutes.\n",
      "485,000 pairs clustered. The previous 1000 pairs took 0.70 minutes.\n",
      "486,000 pairs clustered. The previous 1000 pairs took 0.70 minutes.\n",
      "487,000 pairs clustered. The previous 1000 pairs took 0.74 minutes.\n",
      "488,000 pairs clustered. The previous 1000 pairs took 0.88 minutes.\n",
      "489,000 pairs clustered. The previous 1000 pairs took 1.98 minutes.\n",
      "490,000 pairs clustered. The previous 1000 pairs took 1.42 minutes.\n",
      "491,000 pairs clustered. The previous 1000 pairs took 1.07 minutes.\n",
      "492,000 pairs clustered. The previous 1000 pairs took 0.93 minutes.\n",
      "493,000 pairs clustered. The previous 1000 pairs took 0.71 minutes.\n",
      "494,000 pairs clustered. The previous 1000 pairs took 0.71 minutes.\n",
      "495,000 pairs clustered. The previous 1000 pairs took 0.72 minutes.\n",
      "496,000 pairs clustered. The previous 1000 pairs took 0.72 minutes.\n",
      "497,000 pairs clustered. The previous 1000 pairs took 0.70 minutes.\n",
      "498,000 pairs clustered. The previous 1000 pairs took 0.71 minutes.\n",
      "499,000 pairs clustered. The previous 1000 pairs took 1.48 minutes.\n",
      "500,000 pairs clustered. The previous 1000 pairs took 0.71 minutes.\n",
      "501,000 pairs clustered. The previous 1000 pairs took 0.72 minutes.\n",
      "502,000 pairs clustered. The previous 1000 pairs took 0.71 minutes.\n",
      "503,000 pairs clustered. The previous 1000 pairs took 0.75 minutes.\n",
      "504,000 pairs clustered. The previous 1000 pairs took 0.73 minutes.\n",
      "505,000 pairs clustered. The previous 1000 pairs took 0.72 minutes.\n",
      "506,000 pairs clustered. The previous 1000 pairs took 0.75 minutes.\n",
      "507,000 pairs clustered. The previous 1000 pairs took 0.71 minutes.\n",
      "508,000 pairs clustered. The previous 1000 pairs took 0.72 minutes.\n",
      "509,000 pairs clustered. The previous 1000 pairs took 0.70 minutes.\n",
      "510,000 pairs clustered. The previous 1000 pairs took 0.82 minutes.\n",
      "511,000 pairs clustered. The previous 1000 pairs took 0.74 minutes.\n",
      "512,000 pairs clustered. The previous 1000 pairs took 0.72 minutes.\n",
      "514,000 pairs clustered. The previous 1000 pairs took 0.72 minutes.\n",
      "515,000 pairs clustered. The previous 1000 pairs took 0.93 minutes.\n",
      "516,000 pairs clustered. The previous 1000 pairs took 0.74 minutes.\n",
      "517,000 pairs clustered. The previous 1000 pairs took 0.71 minutes.\n",
      "518,000 pairs clustered. The previous 1000 pairs took 0.76 minutes.\n",
      "519,000 pairs clustered. The previous 1000 pairs took 0.88 minutes.\n",
      "520,000 pairs clustered. The previous 1000 pairs took 0.75 minutes.\n",
      "521,000 pairs clustered. The previous 1000 pairs took 0.74 minutes.\n",
      "522,000 pairs clustered. The previous 1000 pairs took 0.71 minutes.\n",
      "523,000 pairs clustered. The previous 1000 pairs took 0.74 minutes.\n",
      "524,000 pairs clustered. The previous 1000 pairs took 0.88 minutes.\n",
      "525,000 pairs clustered. The previous 1000 pairs took 0.73 minutes.\n",
      "526,000 pairs clustered. The previous 1000 pairs took 0.74 minutes.\n",
      "527,000 pairs clustered. The previous 1000 pairs took 0.71 minutes.\n",
      "528,000 pairs clustered. The previous 1000 pairs took 0.72 minutes.\n",
      "529,000 pairs clustered. The previous 1000 pairs took 0.84 minutes.\n",
      "530,000 pairs clustered. The previous 1000 pairs took 0.78 minutes.\n",
      "531,000 pairs clustered. The previous 1000 pairs took 0.73 minutes.\n",
      "532,000 pairs clustered. The previous 1000 pairs took 0.99 minutes.\n",
      "533,000 pairs clustered. The previous 1000 pairs took 0.73 minutes.\n",
      "534,000 pairs clustered. The previous 1000 pairs took 0.75 minutes.\n",
      "535,000 pairs clustered. The previous 1000 pairs took 1.28 minutes.\n",
      "536,000 pairs clustered. The previous 1000 pairs took 0.70 minutes.\n",
      "537,000 pairs clustered. The previous 1000 pairs took 0.75 minutes.\n",
      "538,000 pairs clustered. The previous 1000 pairs took 0.76 minutes.\n",
      "539,000 pairs clustered. The previous 1000 pairs took 0.76 minutes.\n",
      "540,000 pairs clustered. The previous 1000 pairs took 0.70 minutes.\n",
      "541,000 pairs clustered. The previous 1000 pairs took 5.74 minutes.\n",
      "542,000 pairs clustered. The previous 1000 pairs took 0.70 minutes.\n",
      "543,000 pairs clustered. The previous 1000 pairs took 0.73 minutes.\n",
      "544,000 pairs clustered. The previous 1000 pairs took 0.75 minutes.\n",
      "545,000 pairs clustered. The previous 1000 pairs took 0.76 minutes.\n",
      "546,000 pairs clustered. The previous 1000 pairs took 0.72 minutes.\n",
      "547,000 pairs clustered. The previous 1000 pairs took 0.77 minutes.\n",
      "548,000 pairs clustered. The previous 1000 pairs took 0.78 minutes.\n",
      "549,000 pairs clustered. The previous 1000 pairs took 0.73 minutes.\n",
      "550,000 pairs clustered. The previous 1000 pairs took 0.78 minutes.\n",
      "552,000 pairs clustered. The previous 1000 pairs took 0.73 minutes.\n",
      "553,000 pairs clustered. The previous 1000 pairs took 1.21 minutes.\n",
      "554,000 pairs clustered. The previous 1000 pairs took 0.75 minutes.\n",
      "555,000 pairs clustered. The previous 1000 pairs took 0.75 minutes.\n",
      "556,000 pairs clustered. The previous 1000 pairs took 0.73 minutes.\n",
      "557,000 pairs clustered. The previous 1000 pairs took 0.78 minutes.\n",
      "559,000 pairs clustered. The previous 1000 pairs took 0.75 minutes.\n",
      "560,000 pairs clustered. The previous 1000 pairs took 0.75 minutes.\n",
      "561,000 pairs clustered. The previous 1000 pairs took 0.77 minutes.\n",
      "562,000 pairs clustered. The previous 1000 pairs took 0.75 minutes.\n",
      "563,000 pairs clustered. The previous 1000 pairs took 0.75 minutes.\n",
      "564,000 pairs clustered. The previous 1000 pairs took 7.05 minutes.\n",
      "565,000 pairs clustered. The previous 1000 pairs took 0.76 minutes.\n",
      "566,000 pairs clustered. The previous 1000 pairs took 0.73 minutes.\n",
      "567,000 pairs clustered. The previous 1000 pairs took 0.74 minutes.\n",
      "568,000 pairs clustered. The previous 1000 pairs took 0.77 minutes.\n",
      "569,000 pairs clustered. The previous 1000 pairs took 0.75 minutes.\n",
      "570,000 pairs clustered. The previous 1000 pairs took 0.74 minutes.\n",
      "571,000 pairs clustered. The previous 1000 pairs took 0.78 minutes.\n",
      "572,000 pairs clustered. The previous 1000 pairs took 0.77 minutes.\n",
      "573,000 pairs clustered. The previous 1000 pairs took 0.84 minutes.\n",
      "574,000 pairs clustered. The previous 1000 pairs took 7.02 minutes.\n",
      "575,000 pairs clustered. The previous 1000 pairs took 0.83 minutes.\n",
      "576,000 pairs clustered. The previous 1000 pairs took 0.83 minutes.\n",
      "577,000 pairs clustered. The previous 1000 pairs took 0.74 minutes.\n",
      "578,000 pairs clustered. The previous 1000 pairs took 1.46 minutes.\n",
      "579,000 pairs clustered. The previous 1000 pairs took 0.79 minutes.\n",
      "580,000 pairs clustered. The previous 1000 pairs took 0.88 minutes.\n",
      "581,000 pairs clustered. The previous 1000 pairs took 0.79 minutes.\n",
      "582,000 pairs clustered. The previous 1000 pairs took 0.82 minutes.\n",
      "583,000 pairs clustered. The previous 1000 pairs took 0.78 minutes.\n",
      "584,000 pairs clustered. The previous 1000 pairs took 0.74 minutes.\n",
      "585,000 pairs clustered. The previous 1000 pairs took 0.98 minutes.\n",
      "586,000 pairs clustered. The previous 1000 pairs took 0.81 minutes.\n",
      "587,000 pairs clustered. The previous 1000 pairs took 0.79 minutes.\n",
      "588,000 pairs clustered. The previous 1000 pairs took 0.84 minutes.\n",
      "589,000 pairs clustered. The previous 1000 pairs took 0.78 minutes.\n",
      "591,000 pairs clustered. The previous 1000 pairs took 0.82 minutes.\n",
      "592,000 pairs clustered. The previous 1000 pairs took 0.74 minutes.\n",
      "593,000 pairs clustered. The previous 1000 pairs took 0.77 minutes.\n",
      "594,000 pairs clustered. The previous 1000 pairs took 0.80 minutes.\n",
      "595,000 pairs clustered. The previous 1000 pairs took 0.86 minutes.\n",
      "596,000 pairs clustered. The previous 1000 pairs took 0.78 minutes.\n",
      "597,000 pairs clustered. The previous 1000 pairs took 0.83 minutes.\n",
      "598,000 pairs clustered. The previous 1000 pairs took 0.82 minutes.\n",
      "599,000 pairs clustered. The previous 1000 pairs took 0.83 minutes.\n",
      "600,000 pairs clustered. The previous 1000 pairs took 0.94 minutes.\n",
      "601,000 pairs clustered. The previous 1000 pairs took 1.02 minutes.\n",
      "602,000 pairs clustered. The previous 1000 pairs took 1.04 minutes.\n",
      "604,000 pairs clustered. The previous 1000 pairs took 0.77 minutes.\n",
      "605,000 pairs clustered. The previous 1000 pairs took 0.82 minutes.\n",
      "606,000 pairs clustered. The previous 1000 pairs took 0.85 minutes.\n",
      "607,000 pairs clustered. The previous 1000 pairs took 0.84 minutes.\n",
      "608,000 pairs clustered. The previous 1000 pairs took 0.83 minutes.\n",
      "609,000 pairs clustered. The previous 1000 pairs took 0.85 minutes.\n",
      "610,000 pairs clustered. The previous 1000 pairs took 0.82 minutes.\n",
      "611,000 pairs clustered. The previous 1000 pairs took 0.74 minutes.\n",
      "612,000 pairs clustered. The previous 1000 pairs took 0.80 minutes.\n",
      "613,000 pairs clustered. The previous 1000 pairs took 0.80 minutes.\n",
      "614,000 pairs clustered. The previous 1000 pairs took 0.81 minutes.\n",
      "615,000 pairs clustered. The previous 1000 pairs took 0.81 minutes.\n",
      "616,000 pairs clustered. The previous 1000 pairs took 0.79 minutes.\n",
      "617,000 pairs clustered. The previous 1000 pairs took 0.84 minutes.\n",
      "618,000 pairs clustered. The previous 1000 pairs took 0.93 minutes.\n",
      "619,000 pairs clustered. The previous 1000 pairs took 0.87 minutes.\n",
      "620,000 pairs clustered. The previous 1000 pairs took 0.81 minutes.\n",
      "621,000 pairs clustered. The previous 1000 pairs took 0.83 minutes.\n",
      "622,000 pairs clustered. The previous 1000 pairs took 1.64 minutes.\n",
      "623,000 pairs clustered. The previous 1000 pairs took 0.84 minutes.\n",
      "624,000 pairs clustered. The previous 1000 pairs took 0.95 minutes.\n",
      "625,000 pairs clustered. The previous 1000 pairs took 1.29 minutes.\n",
      "626,000 pairs clustered. The previous 1000 pairs took 0.84 minutes.\n",
      "627,000 pairs clustered. The previous 1000 pairs took 1.40 minutes.\n",
      "628,000 pairs clustered. The previous 1000 pairs took 1.68 minutes.\n",
      "629,000 pairs clustered. The previous 1000 pairs took 0.79 minutes.\n",
      "630,000 pairs clustered. The previous 1000 pairs took 0.78 minutes.\n",
      "631,000 pairs clustered. The previous 1000 pairs took 0.97 minutes.\n",
      "632,000 pairs clustered. The previous 1000 pairs took 0.85 minutes.\n",
      "633,000 pairs clustered. The previous 1000 pairs took 0.81 minutes.\n",
      "634,000 pairs clustered. The previous 1000 pairs took 0.85 minutes.\n",
      "635,000 pairs clustered. The previous 1000 pairs took 0.87 minutes.\n",
      "636,000 pairs clustered. The previous 1000 pairs took 0.94 minutes.\n",
      "637,000 pairs clustered. The previous 1000 pairs took 0.82 minutes.\n",
      "638,000 pairs clustered. The previous 1000 pairs took 0.80 minutes.\n",
      "640,000 pairs clustered. The previous 1000 pairs took 0.84 minutes.\n",
      "641,000 pairs clustered. The previous 1000 pairs took 0.86 minutes.\n",
      "642,000 pairs clustered. The previous 1000 pairs took 1.11 minutes.\n",
      "643,000 pairs clustered. The previous 1000 pairs took 0.89 minutes.\n",
      "644,000 pairs clustered. The previous 1000 pairs took 0.88 minutes.\n",
      "645,000 pairs clustered. The previous 1000 pairs took 0.87 minutes.\n",
      "646,000 pairs clustered. The previous 1000 pairs took 0.88 minutes.\n",
      "647,000 pairs clustered. The previous 1000 pairs took 0.84 minutes.\n",
      "648,000 pairs clustered. The previous 1000 pairs took 0.88 minutes.\n",
      "649,000 pairs clustered. The previous 1000 pairs took 0.88 minutes.\n",
      "650,000 pairs clustered. The previous 1000 pairs took 1.18 minutes.\n",
      "651,000 pairs clustered. The previous 1000 pairs took 1.26 minutes.\n",
      "652,000 pairs clustered. The previous 1000 pairs took 0.89 minutes.\n",
      "653,000 pairs clustered. The previous 1000 pairs took 0.88 minutes.\n",
      "654,000 pairs clustered. The previous 1000 pairs took 0.85 minutes.\n",
      "655,000 pairs clustered. The previous 1000 pairs took 3.55 minutes.\n",
      "656,000 pairs clustered. The previous 1000 pairs took 1.63 minutes.\n",
      "657,000 pairs clustered. The previous 1000 pairs took 0.83 minutes.\n",
      "658,000 pairs clustered. The previous 1000 pairs took 1.01 minutes.\n",
      "659,000 pairs clustered. The previous 1000 pairs took 0.89 minutes.\n",
      "660,000 pairs clustered. The previous 1000 pairs took 0.83 minutes.\n",
      "661,000 pairs clustered. The previous 1000 pairs took 0.99 minutes.\n",
      "662,000 pairs clustered. The previous 1000 pairs took 0.92 minutes.\n",
      "663,000 pairs clustered. The previous 1000 pairs took 1.02 minutes.\n",
      "664,000 pairs clustered. The previous 1000 pairs took 0.90 minutes.\n",
      "665,000 pairs clustered. The previous 1000 pairs took 0.89 minutes.\n",
      "666,000 pairs clustered. The previous 1000 pairs took 0.85 minutes.\n",
      "667,000 pairs clustered. The previous 1000 pairs took 0.79 minutes.\n",
      "668,000 pairs clustered. The previous 1000 pairs took 0.83 minutes.\n",
      "669,000 pairs clustered. The previous 1000 pairs took 5.99 minutes.\n",
      "670,000 pairs clustered. The previous 1000 pairs took 0.82 minutes.\n",
      "671,000 pairs clustered. The previous 1000 pairs took 1.17 minutes.\n",
      "672,000 pairs clustered. The previous 1000 pairs took 0.93 minutes.\n",
      "673,000 pairs clustered. The previous 1000 pairs took 0.89 minutes.\n",
      "674,000 pairs clustered. The previous 1000 pairs took 0.85 minutes.\n",
      "675,000 pairs clustered. The previous 1000 pairs took 16.98 minutes.\n",
      "676,000 pairs clustered. The previous 1000 pairs took 0.79 minutes.\n",
      "677,000 pairs clustered. The previous 1000 pairs took 0.81 minutes.\n",
      "678,000 pairs clustered. The previous 1000 pairs took 0.80 minutes.\n",
      "679,000 pairs clustered. The previous 1000 pairs took 0.81 minutes.\n",
      "680,000 pairs clustered. The previous 1000 pairs took 1.44 minutes.\n",
      "681,000 pairs clustered. The previous 1000 pairs took 1.38 minutes.\n",
      "683,000 pairs clustered. The previous 1000 pairs took 0.80 minutes.\n",
      "684,000 pairs clustered. The previous 1000 pairs took 0.81 minutes.\n",
      "685,000 pairs clustered. The previous 1000 pairs took 0.82 minutes.\n",
      "686,000 pairs clustered. The previous 1000 pairs took 0.79 minutes.\n",
      "687,000 pairs clustered. The previous 1000 pairs took 0.82 minutes.\n",
      "688,000 pairs clustered. The previous 1000 pairs took 0.84 minutes.\n",
      "689,000 pairs clustered. The previous 1000 pairs took 0.83 minutes.\n",
      "690,000 pairs clustered. The previous 1000 pairs took 0.81 minutes.\n",
      "691,000 pairs clustered. The previous 1000 pairs took 1.01 minutes.\n",
      "692,000 pairs clustered. The previous 1000 pairs took 0.80 minutes.\n",
      "693,000 pairs clustered. The previous 1000 pairs took 0.90 minutes.\n",
      "694,000 pairs clustered. The previous 1000 pairs took 0.83 minutes.\n",
      "695,000 pairs clustered. The previous 1000 pairs took 0.79 minutes.\n",
      "696,000 pairs clustered. The previous 1000 pairs took 0.82 minutes.\n",
      "697,000 pairs clustered. The previous 1000 pairs took 0.88 minutes.\n",
      "698,000 pairs clustered. The previous 1000 pairs took 0.83 minutes.\n",
      "699,000 pairs clustered. The previous 1000 pairs took 0.82 minutes.\n",
      "700,000 pairs clustered. The previous 1000 pairs took 0.79 minutes.\n",
      "701,000 pairs clustered. The previous 1000 pairs took 0.79 minutes.\n",
      "702,000 pairs clustered. The previous 1000 pairs took 0.80 minutes.\n",
      "703,000 pairs clustered. The previous 1000 pairs took 0.79 minutes.\n",
      "704,000 pairs clustered. The previous 1000 pairs took 0.85 minutes.\n",
      "705,000 pairs clustered. The previous 1000 pairs took 0.80 minutes.\n",
      "706,000 pairs clustered. The previous 1000 pairs took 0.83 minutes.\n",
      "707,000 pairs clustered. The previous 1000 pairs took 0.77 minutes.\n",
      "708,000 pairs clustered. The previous 1000 pairs took 0.83 minutes.\n",
      "709,000 pairs clustered. The previous 1000 pairs took 0.83 minutes.\n",
      "710,000 pairs clustered. The previous 1000 pairs took 0.93 minutes.\n",
      "711,000 pairs clustered. The previous 1000 pairs took 0.88 minutes.\n",
      "712,000 pairs clustered. The previous 1000 pairs took 0.85 minutes.\n",
      "713,000 pairs clustered. The previous 1000 pairs took 0.84 minutes.\n",
      "714,000 pairs clustered. The previous 1000 pairs took 0.84 minutes.\n",
      "715,000 pairs clustered. The previous 1000 pairs took 0.96 minutes.\n",
      "716,000 pairs clustered. The previous 1000 pairs took 0.92 minutes.\n",
      "717,000 pairs clustered. The previous 1000 pairs took 0.83 minutes.\n",
      "718,000 pairs clustered. The previous 1000 pairs took 0.84 minutes.\n",
      "719,000 pairs clustered. The previous 1000 pairs took 0.83 minutes.\n",
      "720,000 pairs clustered. The previous 1000 pairs took 0.82 minutes.\n",
      "721,000 pairs clustered. The previous 1000 pairs took 1.07 minutes.\n",
      "722,000 pairs clustered. The previous 1000 pairs took 0.85 minutes.\n",
      "723,000 pairs clustered. The previous 1000 pairs took 0.87 minutes.\n",
      "724,000 pairs clustered. The previous 1000 pairs took 0.89 minutes.\n",
      "725,000 pairs clustered. The previous 1000 pairs took 0.88 minutes.\n",
      "726,000 pairs clustered. The previous 1000 pairs took 1.47 minutes.\n",
      "727,000 pairs clustered. The previous 1000 pairs took 0.86 minutes.\n",
      "728,000 pairs clustered. The previous 1000 pairs took 0.88 minutes.\n",
      "729,000 pairs clustered. The previous 1000 pairs took 0.88 minutes.\n",
      "730,000 pairs clustered. The previous 1000 pairs took 1.04 minutes.\n",
      "731,000 pairs clustered. The previous 1000 pairs took 0.88 minutes.\n",
      "732,000 pairs clustered. The previous 1000 pairs took 0.88 minutes.\n",
      "733,000 pairs clustered. The previous 1000 pairs took 1.00 minutes.\n",
      "734,000 pairs clustered. The previous 1000 pairs took 1.20 minutes.\n",
      "735,000 pairs clustered. The previous 1000 pairs took 1.15 minutes.\n",
      "737,000 pairs clustered. The previous 1000 pairs took 0.87 minutes.\n",
      "738,000 pairs clustered. The previous 1000 pairs took 0.88 minutes.\n",
      "739,000 pairs clustered. The previous 1000 pairs took 0.89 minutes.\n",
      "740,000 pairs clustered. The previous 1000 pairs took 0.91 minutes.\n",
      "741,000 pairs clustered. The previous 1000 pairs took 0.91 minutes.\n",
      "742,000 pairs clustered. The previous 1000 pairs took 0.97 minutes.\n",
      "743,000 pairs clustered. The previous 1000 pairs took 1.01 minutes.\n",
      "744,000 pairs clustered. The previous 1000 pairs took 0.89 minutes.\n",
      "745,000 pairs clustered. The previous 1000 pairs took 0.89 minutes.\n",
      "746,000 pairs clustered. The previous 1000 pairs took 0.91 minutes.\n",
      "747,000 pairs clustered. The previous 1000 pairs took 0.88 minutes.\n",
      "748,000 pairs clustered. The previous 1000 pairs took 0.91 minutes.\n",
      "749,000 pairs clustered. The previous 1000 pairs took 0.90 minutes.\n",
      "750,000 pairs clustered. The previous 1000 pairs took 0.91 minutes.\n",
      "751,000 pairs clustered. The previous 1000 pairs took 0.90 minutes.\n",
      "752,000 pairs clustered. The previous 1000 pairs took 1.05 minutes.\n",
      "753,000 pairs clustered. The previous 1000 pairs took 0.89 minutes.\n",
      "754,000 pairs clustered. The previous 1000 pairs took 0.88 minutes.\n",
      "755,000 pairs clustered. The previous 1000 pairs took 0.90 minutes.\n",
      "756,000 pairs clustered. The previous 1000 pairs took 1.09 minutes.\n",
      "757,000 pairs clustered. The previous 1000 pairs took 0.88 minutes.\n",
      "758,000 pairs clustered. The previous 1000 pairs took 0.88 minutes.\n",
      "759,000 pairs clustered. The previous 1000 pairs took 0.91 minutes.\n",
      "760,000 pairs clustered. The previous 1000 pairs took 0.91 minutes.\n",
      "761,000 pairs clustered. The previous 1000 pairs took 0.89 minutes.\n",
      "762,000 pairs clustered. The previous 1000 pairs took 0.92 minutes.\n",
      "763,000 pairs clustered. The previous 1000 pairs took 0.92 minutes.\n",
      "764,000 pairs clustered. The previous 1000 pairs took 0.94 minutes.\n",
      "765,000 pairs clustered. The previous 1000 pairs took 0.92 minutes.\n",
      "766,000 pairs clustered. The previous 1000 pairs took 1.08 minutes.\n",
      "767,000 pairs clustered. The previous 1000 pairs took 1.20 minutes.\n",
      "768,000 pairs clustered. The previous 1000 pairs took 1.00 minutes.\n",
      "770,000 pairs clustered. The previous 1000 pairs took 0.90 minutes.\n",
      "771,000 pairs clustered. The previous 1000 pairs took 0.89 minutes.\n",
      "772,000 pairs clustered. The previous 1000 pairs took 0.92 minutes.\n",
      "773,000 pairs clustered. The previous 1000 pairs took 0.90 minutes.\n",
      "774,000 pairs clustered. The previous 1000 pairs took 0.92 minutes.\n",
      "775,000 pairs clustered. The previous 1000 pairs took 0.92 minutes.\n",
      "776,000 pairs clustered. The previous 1000 pairs took 0.92 minutes.\n",
      "777,000 pairs clustered. The previous 1000 pairs took 0.96 minutes.\n",
      "778,000 pairs clustered. The previous 1000 pairs took 0.94 minutes.\n",
      "779,000 pairs clustered. The previous 1000 pairs took 1.16 minutes.\n",
      "780,000 pairs clustered. The previous 1000 pairs took 1.24 minutes.\n",
      "781,000 pairs clustered. The previous 1000 pairs took 0.89 minutes.\n",
      "782,000 pairs clustered. The previous 1000 pairs took 2.13 minutes.\n",
      "783,000 pairs clustered. The previous 1000 pairs took 0.93 minutes.\n",
      "784,000 pairs clustered. The previous 1000 pairs took 0.94 minutes.\n",
      "785,000 pairs clustered. The previous 1000 pairs took 0.91 minutes.\n",
      "786,000 pairs clustered. The previous 1000 pairs took 0.92 minutes.\n",
      "787,000 pairs clustered. The previous 1000 pairs took 1.01 minutes.\n",
      "788,000 pairs clustered. The previous 1000 pairs took 0.92 minutes.\n",
      "789,000 pairs clustered. The previous 1000 pairs took 0.93 minutes.\n",
      "790,000 pairs clustered. The previous 1000 pairs took 0.91 minutes.\n",
      "792,000 pairs clustered. The previous 1000 pairs took 0.91 minutes.\n",
      "793,000 pairs clustered. The previous 1000 pairs took 0.94 minutes.\n",
      "794,000 pairs clustered. The previous 1000 pairs took 0.93 minutes.\n",
      "795,000 pairs clustered. The previous 1000 pairs took 0.91 minutes.\n",
      "796,000 pairs clustered. The previous 1000 pairs took 0.93 minutes.\n",
      "797,000 pairs clustered. The previous 1000 pairs took 0.99 minutes.\n",
      "798,000 pairs clustered. The previous 1000 pairs took 0.95 minutes.\n",
      "800,000 pairs clustered. The previous 1000 pairs took 0.99 minutes.\n",
      "801,000 pairs clustered. The previous 1000 pairs took 0.98 minutes.\n",
      "802,000 pairs clustered. The previous 1000 pairs took 0.94 minutes.\n",
      "803,000 pairs clustered. The previous 1000 pairs took 0.94 minutes.\n",
      "804,000 pairs clustered. The previous 1000 pairs took 0.94 minutes.\n",
      "805,000 pairs clustered. The previous 1000 pairs took 0.97 minutes.\n",
      "806,000 pairs clustered. The previous 1000 pairs took 0.93 minutes.\n",
      "807,000 pairs clustered. The previous 1000 pairs took 0.95 minutes.\n",
      "808,000 pairs clustered. The previous 1000 pairs took 0.95 minutes.\n",
      "809,000 pairs clustered. The previous 1000 pairs took 0.95 minutes.\n",
      "810,000 pairs clustered. The previous 1000 pairs took 0.92 minutes.\n",
      "811,000 pairs clustered. The previous 1000 pairs took 0.93 minutes.\n",
      "812,000 pairs clustered. The previous 1000 pairs took 0.92 minutes.\n",
      "813,000 pairs clustered. The previous 1000 pairs took 0.96 minutes.\n",
      "814,000 pairs clustered. The previous 1000 pairs took 1.00 minutes.\n",
      "815,000 pairs clustered. The previous 1000 pairs took 7.36 minutes.\n",
      "816,000 pairs clustered. The previous 1000 pairs took 1.00 minutes.\n",
      "817,000 pairs clustered. The previous 1000 pairs took 0.96 minutes.\n",
      "818,000 pairs clustered. The previous 1000 pairs took 0.98 minutes.\n",
      "819,000 pairs clustered. The previous 1000 pairs took 0.98 minutes.\n",
      "820,000 pairs clustered. The previous 1000 pairs took 0.95 minutes.\n",
      "821,000 pairs clustered. The previous 1000 pairs took 0.94 minutes.\n",
      "822,000 pairs clustered. The previous 1000 pairs took 1.50 minutes.\n",
      "823,000 pairs clustered. The previous 1000 pairs took 0.96 minutes.\n",
      "824,000 pairs clustered. The previous 1000 pairs took 0.95 minutes.\n",
      "825,000 pairs clustered. The previous 1000 pairs took 1.06 minutes.\n",
      "826,000 pairs clustered. The previous 1000 pairs took 0.95 minutes.\n",
      "827,000 pairs clustered. The previous 1000 pairs took 0.96 minutes.\n",
      "828,000 pairs clustered. The previous 1000 pairs took 0.97 minutes.\n",
      "829,000 pairs clustered. The previous 1000 pairs took 1.01 minutes.\n",
      "830,000 pairs clustered. The previous 1000 pairs took 0.97 minutes.\n",
      "831,000 pairs clustered. The previous 1000 pairs took 0.96 minutes.\n",
      "832,000 pairs clustered. The previous 1000 pairs took 0.99 minutes.\n",
      "833,000 pairs clustered. The previous 1000 pairs took 0.96 minutes.\n",
      "834,000 pairs clustered. The previous 1000 pairs took 0.98 minutes.\n",
      "835,000 pairs clustered. The previous 1000 pairs took 0.95 minutes.\n",
      "836,000 pairs clustered. The previous 1000 pairs took 1.76 minutes.\n",
      "837,000 pairs clustered. The previous 1000 pairs took 1.00 minutes.\n",
      "838,000 pairs clustered. The previous 1000 pairs took 0.98 minutes.\n",
      "839,000 pairs clustered. The previous 1000 pairs took 0.98 minutes.\n",
      "840,000 pairs clustered. The previous 1000 pairs took 0.98 minutes.\n",
      "841,000 pairs clustered. The previous 1000 pairs took 1.09 minutes.\n",
      "842,000 pairs clustered. The previous 1000 pairs took 0.96 minutes.\n",
      "843,000 pairs clustered. The previous 1000 pairs took 1.00 minutes.\n",
      "844,000 pairs clustered. The previous 1000 pairs took 0.97 minutes.\n",
      "845,000 pairs clustered. The previous 1000 pairs took 0.97 minutes.\n",
      "846,000 pairs clustered. The previous 1000 pairs took 0.97 minutes.\n",
      "847,000 pairs clustered. The previous 1000 pairs took 0.98 minutes.\n",
      "848,000 pairs clustered. The previous 1000 pairs took 1.79 minutes.\n",
      "849,000 pairs clustered. The previous 1000 pairs took 0.88 minutes.\n",
      "850,000 pairs clustered. The previous 1000 pairs took 0.94 minutes.\n",
      "851,000 pairs clustered. The previous 1000 pairs took 1.01 minutes.\n",
      "852,000 pairs clustered. The previous 1000 pairs took 0.89 minutes.\n",
      "853,000 pairs clustered. The previous 1000 pairs took 1.73 minutes.\n",
      "854,000 pairs clustered. The previous 1000 pairs took 0.93 minutes.\n",
      "855,000 pairs clustered. The previous 1000 pairs took 0.89 minutes.\n",
      "856,000 pairs clustered. The previous 1000 pairs took 0.88 minutes.\n",
      "857,000 pairs clustered. The previous 1000 pairs took 0.92 minutes.\n",
      "858,000 pairs clustered. The previous 1000 pairs took 0.90 minutes.\n",
      "859,000 pairs clustered. The previous 1000 pairs took 1.18 minutes.\n",
      "860,000 pairs clustered. The previous 1000 pairs took 0.92 minutes.\n",
      "862,000 pairs clustered. The previous 1000 pairs took 0.90 minutes.\n",
      "863,000 pairs clustered. The previous 1000 pairs took 1.15 minutes.\n",
      "864,000 pairs clustered. The previous 1000 pairs took 0.94 minutes.\n",
      "865,000 pairs clustered. The previous 1000 pairs took 0.92 minutes.\n",
      "866,000 pairs clustered. The previous 1000 pairs took 0.95 minutes.\n",
      "867,000 pairs clustered. The previous 1000 pairs took 0.90 minutes.\n",
      "868,000 pairs clustered. The previous 1000 pairs took 1.03 minutes.\n",
      "869,000 pairs clustered. The previous 1000 pairs took 0.95 minutes.\n",
      "870,000 pairs clustered. The previous 1000 pairs took 0.97 minutes.\n",
      "871,000 pairs clustered. The previous 1000 pairs took 2.49 minutes.\n",
      "872,000 pairs clustered. The previous 1000 pairs took 1.00 minutes.\n",
      "873,000 pairs clustered. The previous 1000 pairs took 0.99 minutes.\n",
      "874,000 pairs clustered. The previous 1000 pairs took 1.10 minutes.\n",
      "875,000 pairs clustered. The previous 1000 pairs took 0.96 minutes.\n",
      "876,000 pairs clustered. The previous 1000 pairs took 0.96 minutes.\n",
      "877,000 pairs clustered. The previous 1000 pairs took 1.04 minutes.\n",
      "878,000 pairs clustered. The previous 1000 pairs took 0.96 minutes.\n",
      "879,000 pairs clustered. The previous 1000 pairs took 1.04 minutes.\n",
      "880,000 pairs clustered. The previous 1000 pairs took 1.16 minutes.\n",
      "881,000 pairs clustered. The previous 1000 pairs took 0.95 minutes.\n",
      "882,000 pairs clustered. The previous 1000 pairs took 0.97 minutes.\n",
      "883,000 pairs clustered. The previous 1000 pairs took 1.00 minutes.\n",
      "884,000 pairs clustered. The previous 1000 pairs took 1.66 minutes.\n",
      "885,000 pairs clustered. The previous 1000 pairs took 0.97 minutes.\n",
      "886,000 pairs clustered. The previous 1000 pairs took 2.38 minutes.\n",
      "887,000 pairs clustered. The previous 1000 pairs took 1.05 minutes.\n",
      "888,000 pairs clustered. The previous 1000 pairs took 1.01 minutes.\n",
      "889,000 pairs clustered. The previous 1000 pairs took 0.97 minutes.\n",
      "890,000 pairs clustered. The previous 1000 pairs took 0.99 minutes.\n",
      "891,000 pairs clustered. The previous 1000 pairs took 0.99 minutes.\n",
      "892,000 pairs clustered. The previous 1000 pairs took 0.99 minutes.\n",
      "893,000 pairs clustered. The previous 1000 pairs took 2.15 minutes.\n",
      "894,000 pairs clustered. The previous 1000 pairs took 1.59 minutes.\n",
      "895,000 pairs clustered. The previous 1000 pairs took 0.99 minutes.\n",
      "896,000 pairs clustered. The previous 1000 pairs took 1.01 minutes.\n",
      "897,000 pairs clustered. The previous 1000 pairs took 0.95 minutes.\n",
      "898,000 pairs clustered. The previous 1000 pairs took 1.07 minutes.\n",
      "899,000 pairs clustered. The previous 1000 pairs took 1.00 minutes.\n",
      "900,000 pairs clustered. The previous 1000 pairs took 1.01 minutes.\n",
      "901,000 pairs clustered. The previous 1000 pairs took 1.14 minutes.\n",
      "902,000 pairs clustered. The previous 1000 pairs took 1.02 minutes.\n",
      "903,000 pairs clustered. The previous 1000 pairs took 1.11 minutes.\n",
      "904,000 pairs clustered. The previous 1000 pairs took 1.05 minutes.\n",
      "905,000 pairs clustered. The previous 1000 pairs took 1.05 minutes.\n",
      "906,000 pairs clustered. The previous 1000 pairs took 1.05 minutes.\n",
      "907,000 pairs clustered. The previous 1000 pairs took 1.33 minutes.\n",
      "908,000 pairs clustered. The previous 1000 pairs took 1.08 minutes.\n",
      "909,000 pairs clustered. The previous 1000 pairs took 1.36 minutes.\n",
      "910,000 pairs clustered. The previous 1000 pairs took 1.49 minutes.\n",
      "911,000 pairs clustered. The previous 1000 pairs took 1.02 minutes.\n",
      "912,000 pairs clustered. The previous 1000 pairs took 1.07 minutes.\n",
      "913,000 pairs clustered. The previous 1000 pairs took 1.09 minutes.\n",
      "914,000 pairs clustered. The previous 1000 pairs took 1.10 minutes.\n",
      "915,000 pairs clustered. The previous 1000 pairs took 1.27 minutes.\n",
      "916,000 pairs clustered. The previous 1000 pairs took 1.06 minutes.\n",
      "917,000 pairs clustered. The previous 1000 pairs took 1.06 minutes.\n",
      "918,000 pairs clustered. The previous 1000 pairs took 1.04 minutes.\n",
      "919,000 pairs clustered. The previous 1000 pairs took 1.08 minutes.\n",
      "920,000 pairs clustered. The previous 1000 pairs took 1.11 minutes.\n",
      "922,000 pairs clustered. The previous 1000 pairs took 1.03 minutes.\n",
      "923,000 pairs clustered. The previous 1000 pairs took 1.04 minutes.\n",
      "924,000 pairs clustered. The previous 1000 pairs took 2.15 minutes.\n",
      "926,000 pairs clustered. The previous 1000 pairs took 1.14 minutes.\n",
      "927,000 pairs clustered. The previous 1000 pairs took 1.03 minutes.\n",
      "928,000 pairs clustered. The previous 1000 pairs took 1.08 minutes.\n",
      "929,000 pairs clustered. The previous 1000 pairs took 1.08 minutes.\n",
      "930,000 pairs clustered. The previous 1000 pairs took 1.07 minutes.\n",
      "931,000 pairs clustered. The previous 1000 pairs took 1.07 minutes.\n",
      "932,000 pairs clustered. The previous 1000 pairs took 1.05 minutes.\n",
      "933,000 pairs clustered. The previous 1000 pairs took 1.05 minutes.\n",
      "934,000 pairs clustered. The previous 1000 pairs took 1.82 minutes.\n",
      "935,000 pairs clustered. The previous 1000 pairs took 1.09 minutes.\n",
      "936,000 pairs clustered. The previous 1000 pairs took 1.08 minutes.\n",
      "937,000 pairs clustered. The previous 1000 pairs took 1.07 minutes.\n",
      "938,000 pairs clustered. The previous 1000 pairs took 1.58 minutes.\n",
      "940,000 pairs clustered. The previous 1000 pairs took 3.01 minutes.\n",
      "941,000 pairs clustered. The previous 1000 pairs took 1.09 minutes.\n",
      "943,000 pairs clustered. The previous 1000 pairs took 1.08 minutes.\n",
      "944,000 pairs clustered. The previous 1000 pairs took 1.08 minutes.\n",
      "945,000 pairs clustered. The previous 1000 pairs took 1.17 minutes.\n",
      "946,000 pairs clustered. The previous 1000 pairs took 1.11 minutes.\n",
      "947,000 pairs clustered. The previous 1000 pairs took 1.10 minutes.\n",
      "948,000 pairs clustered. The previous 1000 pairs took 1.10 minutes.\n",
      "949,000 pairs clustered. The previous 1000 pairs took 1.27 minutes.\n",
      "950,000 pairs clustered. The previous 1000 pairs took 1.10 minutes.\n",
      "951,000 pairs clustered. The previous 1000 pairs took 1.13 minutes.\n",
      "953,000 pairs clustered. The previous 1000 pairs took 1.23 minutes.\n",
      "954,000 pairs clustered. The previous 1000 pairs took 1.13 minutes.\n",
      "955,000 pairs clustered. The previous 1000 pairs took 1.51 minutes.\n",
      "956,000 pairs clustered. The previous 1000 pairs took 1.13 minutes.\n",
      "957,000 pairs clustered. The previous 1000 pairs took 1.15 minutes.\n",
      "958,000 pairs clustered. The previous 1000 pairs took 1.13 minutes.\n",
      "959,000 pairs clustered. The previous 1000 pairs took 1.13 minutes.\n",
      "960,000 pairs clustered. The previous 1000 pairs took 1.15 minutes.\n",
      "961,000 pairs clustered. The previous 1000 pairs took 1.11 minutes.\n",
      "962,000 pairs clustered. The previous 1000 pairs took 1.13 minutes.\n",
      "963,000 pairs clustered. The previous 1000 pairs took 1.12 minutes.\n",
      "964,000 pairs clustered. The previous 1000 pairs took 1.14 minutes.\n",
      "965,000 pairs clustered. The previous 1000 pairs took 1.28 minutes.\n",
      "966,000 pairs clustered. The previous 1000 pairs took 1.39 minutes.\n",
      "967,000 pairs clustered. The previous 1000 pairs took 1.13 minutes.\n",
      "968,000 pairs clustered. The previous 1000 pairs took 1.35 minutes.\n",
      "969,000 pairs clustered. The previous 1000 pairs took 6.46 minutes.\n",
      "970,000 pairs clustered. The previous 1000 pairs took 1.18 minutes.\n",
      "971,000 pairs clustered. The previous 1000 pairs took 1.17 minutes.\n",
      "972,000 pairs clustered. The previous 1000 pairs took 1.25 minutes.\n",
      "973,000 pairs clustered. The previous 1000 pairs took 1.16 minutes.\n",
      "974,000 pairs clustered. The previous 1000 pairs took 1.20 minutes.\n",
      "976,000 pairs clustered. The previous 1000 pairs took 1.27 minutes.\n",
      "977,000 pairs clustered. The previous 1000 pairs took 1.15 minutes.\n",
      "978,000 pairs clustered. The previous 1000 pairs took 1.15 minutes.\n",
      "979,000 pairs clustered. The previous 1000 pairs took 1.14 minutes.\n",
      "980,000 pairs clustered. The previous 1000 pairs took 1.18 minutes.\n",
      "981,000 pairs clustered. The previous 1000 pairs took 1.16 minutes.\n",
      "982,000 pairs clustered. The previous 1000 pairs took 1.36 minutes.\n",
      "983,000 pairs clustered. The previous 1000 pairs took 1.23 minutes.\n",
      "984,000 pairs clustered. The previous 1000 pairs took 1.23 minutes.\n",
      "985,000 pairs clustered. The previous 1000 pairs took 1.20 minutes.\n",
      "986,000 pairs clustered. The previous 1000 pairs took 1.29 minutes.\n",
      "987,000 pairs clustered. The previous 1000 pairs took 12.43 minutes.\n",
      "988,000 pairs clustered. The previous 1000 pairs took 1.21 minutes.\n",
      "989,000 pairs clustered. The previous 1000 pairs took 1.17 minutes.\n",
      "990,000 pairs clustered. The previous 1000 pairs took 1.26 minutes.\n",
      "991,000 pairs clustered. The previous 1000 pairs took 1.20 minutes.\n",
      "992,000 pairs clustered. The previous 1000 pairs took 1.21 minutes.\n",
      "993,000 pairs clustered. The previous 1000 pairs took 1.26 minutes.\n",
      "994,000 pairs clustered. The previous 1000 pairs took 1.27 minutes.\n",
      "995,000 pairs clustered. The previous 1000 pairs took 1.38 minutes.\n",
      "996,000 pairs clustered. The previous 1000 pairs took 1.22 minutes.\n",
      "997,000 pairs clustered. The previous 1000 pairs took 1.28 minutes.\n",
      "998,000 pairs clustered. The previous 1000 pairs took 1.23 minutes.\n",
      "999,000 pairs clustered. The previous 1000 pairs took 1.40 minutes.\n",
      "1,000,000 pairs clustered. The previous 1000 pairs took 1.26 minutes.\n",
      "1,001,000 pairs clustered. The previous 1000 pairs took 1.83 minutes.\n",
      "1,002,000 pairs clustered. The previous 1000 pairs took 1.38 minutes.\n",
      "1,003,000 pairs clustered. The previous 1000 pairs took 1.43 minutes.\n",
      "1,004,000 pairs clustered. The previous 1000 pairs took 1.30 minutes.\n",
      "1,005,000 pairs clustered. The previous 1000 pairs took 1.27 minutes.\n",
      "1,006,000 pairs clustered. The previous 1000 pairs took 2.60 minutes.\n",
      "1,007,000 pairs clustered. The previous 1000 pairs took 1.27 minutes.\n",
      "1,008,000 pairs clustered. The previous 1000 pairs took 1.28 minutes.\n",
      "1,009,000 pairs clustered. The previous 1000 pairs took 1.29 minutes.\n",
      "1,010,000 pairs clustered. The previous 1000 pairs took 1.29 minutes.\n",
      "1,011,000 pairs clustered. The previous 1000 pairs took 1.32 minutes.\n",
      "1,012,000 pairs clustered. The previous 1000 pairs took 1.27 minutes.\n",
      "1,013,000 pairs clustered. The previous 1000 pairs took 1.32 minutes.\n",
      "1,014,000 pairs clustered. The previous 1000 pairs took 1.49 minutes.\n",
      "1,015,000 pairs clustered. The previous 1000 pairs took 1.25 minutes.\n",
      "1,016,000 pairs clustered. The previous 1000 pairs took 1.35 minutes.\n",
      "1,017,000 pairs clustered. The previous 1000 pairs took 1.32 minutes.\n",
      "1,018,000 pairs clustered. The previous 1000 pairs took 1.29 minutes.\n",
      "1,019,000 pairs clustered. The previous 1000 pairs took 1.31 minutes.\n",
      "1,020,000 pairs clustered. The previous 1000 pairs took 1.39 minutes.\n",
      "1,021,000 pairs clustered. The previous 1000 pairs took 1.30 minutes.\n",
      "1,022,000 pairs clustered. The previous 1000 pairs took 1.32 minutes.\n",
      "1,023,000 pairs clustered. The previous 1000 pairs took 1.32 minutes.\n",
      "1,024,000 pairs clustered. The previous 1000 pairs took 1.38 minutes.\n",
      "1,025,000 pairs clustered. The previous 1000 pairs took 1.38 minutes.\n",
      "1,026,000 pairs clustered. The previous 1000 pairs took 1.36 minutes.\n",
      "1,027,000 pairs clustered. The previous 1000 pairs took 1.34 minutes.\n",
      "1,028,000 pairs clustered. The previous 1000 pairs took 1.37 minutes.\n",
      "1,029,000 pairs clustered. The previous 1000 pairs took 1.37 minutes.\n",
      "1,030,000 pairs clustered. The previous 1000 pairs took 1.36 minutes.\n",
      "1,031,000 pairs clustered. The previous 1000 pairs took 1.44 minutes.\n",
      "1,032,000 pairs clustered. The previous 1000 pairs took 1.42 minutes.\n",
      "1,033,000 pairs clustered. The previous 1000 pairs took 1.38 minutes.\n",
      "1,034,000 pairs clustered. The previous 1000 pairs took 1.49 minutes.\n",
      "1,035,000 pairs clustered. The previous 1000 pairs took 1.35 minutes.\n",
      "1,036,000 pairs clustered. The previous 1000 pairs took 1.34 minutes.\n",
      "1,037,000 pairs clustered. The previous 1000 pairs took 15.53 minutes.\n",
      "1,038,000 pairs clustered. The previous 1000 pairs took 1.37 minutes.\n",
      "1,039,000 pairs clustered. The previous 1000 pairs took 1.54 minutes.\n",
      "1,040,000 pairs clustered. The previous 1000 pairs took 1.37 minutes.\n",
      "1,041,000 pairs clustered. The previous 1000 pairs took 1.40 minutes.\n",
      "1,042,000 pairs clustered. The previous 1000 pairs took 1.38 minutes.\n",
      "1,043,000 pairs clustered. The previous 1000 pairs took 1.40 minutes.\n",
      "1,044,000 pairs clustered. The previous 1000 pairs took 1.40 minutes.\n",
      "1,045,000 pairs clustered. The previous 1000 pairs took 1.33 minutes.\n",
      "1,046,000 pairs clustered. The previous 1000 pairs took 1.36 minutes.\n",
      "1,047,000 pairs clustered. The previous 1000 pairs took 1.43 minutes.\n",
      "1,049,000 pairs clustered. The previous 1000 pairs took 1.43 minutes.\n",
      "1,050,000 pairs clustered. The previous 1000 pairs took 1.35 minutes.\n",
      "1,051,000 pairs clustered. The previous 1000 pairs took 1.32 minutes.\n",
      "1,052,000 pairs clustered. The previous 1000 pairs took 1.32 minutes.\n",
      "1,053,000 pairs clustered. The previous 1000 pairs took 1.48 minutes.\n",
      "1,054,000 pairs clustered. The previous 1000 pairs took 1.34 minutes.\n",
      "1,055,000 pairs clustered. The previous 1000 pairs took 1.33 minutes.\n",
      "1,056,000 pairs clustered. The previous 1000 pairs took 1.34 minutes.\n",
      "1,057,000 pairs clustered. The previous 1000 pairs took 1.35 minutes.\n",
      "1,058,000 pairs clustered. The previous 1000 pairs took 1.35 minutes.\n",
      "1,059,000 pairs clustered. The previous 1000 pairs took 2.67 minutes.\n",
      "1,060,000 pairs clustered. The previous 1000 pairs took 1.42 minutes.\n",
      "1,061,000 pairs clustered. The previous 1000 pairs took 1.34 minutes.\n",
      "1,062,000 pairs clustered. The previous 1000 pairs took 1.36 minutes.\n",
      "1,063,000 pairs clustered. The previous 1000 pairs took 1.57 minutes.\n",
      "1,064,000 pairs clustered. The previous 1000 pairs took 1.35 minutes.\n",
      "1,065,000 pairs clustered. The previous 1000 pairs took 1.36 minutes.\n",
      "1,066,000 pairs clustered. The previous 1000 pairs took 1.34 minutes.\n",
      "1,067,000 pairs clustered. The previous 1000 pairs took 1.51 minutes.\n",
      "1,068,000 pairs clustered. The previous 1000 pairs took 1.29 minutes.\n",
      "1,069,000 pairs clustered. The previous 1000 pairs took 1.32 minutes.\n",
      "1,071,000 pairs clustered. The previous 1000 pairs took 1.33 minutes.\n",
      "1,072,000 pairs clustered. The previous 1000 pairs took 1.37 minutes.\n",
      "1,073,000 pairs clustered. The previous 1000 pairs took 1.35 minutes.\n",
      "1,074,000 pairs clustered. The previous 1000 pairs took 1.76 minutes.\n",
      "1,075,000 pairs clustered. The previous 1000 pairs took 1.33 minutes.\n",
      "1,077,000 pairs clustered. The previous 1000 pairs took 1.53 minutes.\n",
      "1,078,000 pairs clustered. The previous 1000 pairs took 1.34 minutes.\n",
      "1,079,000 pairs clustered. The previous 1000 pairs took 1.36 minutes.\n",
      "1,080,000 pairs clustered. The previous 1000 pairs took 1.32 minutes.\n",
      "1,081,000 pairs clustered. The previous 1000 pairs took 1.31 minutes.\n",
      "1,082,000 pairs clustered. The previous 1000 pairs took 1.38 minutes.\n",
      "1,083,000 pairs clustered. The previous 1000 pairs took 1.44 minutes.\n",
      "1,084,000 pairs clustered. The previous 1000 pairs took 1.37 minutes.\n",
      "1,085,000 pairs clustered. The previous 1000 pairs took 2.32 minutes.\n",
      "1,086,000 pairs clustered. The previous 1000 pairs took 1.36 minutes.\n",
      "1,088,000 pairs clustered. The previous 1000 pairs took 1.35 minutes.\n",
      "1,089,000 pairs clustered. The previous 1000 pairs took 1.47 minutes.\n",
      "1,090,000 pairs clustered. The previous 1000 pairs took 1.35 minutes.\n",
      "1,091,000 pairs clustered. The previous 1000 pairs took 1.38 minutes.\n",
      "1,092,000 pairs clustered. The previous 1000 pairs took 1.42 minutes.\n",
      "1,093,000 pairs clustered. The previous 1000 pairs took 1.39 minutes.\n",
      "1,094,000 pairs clustered. The previous 1000 pairs took 1.40 minutes.\n",
      "1,095,000 pairs clustered. The previous 1000 pairs took 1.40 minutes.\n",
      "1,096,000 pairs clustered. The previous 1000 pairs took 1.41 minutes.\n",
      "1,097,000 pairs clustered. The previous 1000 pairs took 1.46 minutes.\n",
      "1,098,000 pairs clustered. The previous 1000 pairs took 1.42 minutes.\n",
      "1,099,000 pairs clustered. The previous 1000 pairs took 1.39 minutes.\n",
      "1,100,000 pairs clustered. The previous 1000 pairs took 1.39 minutes.\n",
      "1,101,000 pairs clustered. The previous 1000 pairs took 1.44 minutes.\n",
      "1,103,000 pairs clustered. The previous 1000 pairs took 1.45 minutes.\n",
      "1,104,000 pairs clustered. The previous 1000 pairs took 1.34 minutes.\n",
      "1,105,000 pairs clustered. The previous 1000 pairs took 1.57 minutes.\n",
      "1,106,000 pairs clustered. The previous 1000 pairs took 1.37 minutes.\n",
      "1,107,000 pairs clustered. The previous 1000 pairs took 1.37 minutes.\n",
      "1,108,000 pairs clustered. The previous 1000 pairs took 1.46 minutes.\n",
      "1,109,000 pairs clustered. The previous 1000 pairs took 1.48 minutes.\n",
      "1,110,000 pairs clustered. The previous 1000 pairs took 1.60 minutes.\n",
      "1,111,000 pairs clustered. The previous 1000 pairs took 2.35 minutes.\n",
      "1,112,000 pairs clustered. The previous 1000 pairs took 1.39 minutes.\n",
      "1,113,000 pairs clustered. The previous 1000 pairs took 1.44 minutes.\n",
      "1,114,000 pairs clustered. The previous 1000 pairs took 1.49 minutes.\n",
      "1,115,000 pairs clustered. The previous 1000 pairs took 1.38 minutes.\n",
      "1,116,000 pairs clustered. The previous 1000 pairs took 1.40 minutes.\n",
      "1,117,000 pairs clustered. The previous 1000 pairs took 1.40 minutes.\n",
      "1,119,000 pairs clustered. The previous 1000 pairs took 1.41 minutes.\n",
      "1,120,000 pairs clustered. The previous 1000 pairs took 1.50 minutes.\n",
      "1,121,000 pairs clustered. The previous 1000 pairs took 1.44 minutes.\n",
      "1,122,000 pairs clustered. The previous 1000 pairs took 1.42 minutes.\n",
      "1,123,000 pairs clustered. The previous 1000 pairs took 1.56 minutes.\n",
      "1,124,000 pairs clustered. The previous 1000 pairs took 2.31 minutes.\n",
      "1,125,000 pairs clustered. The previous 1000 pairs took 1.43 minutes.\n",
      "1,126,000 pairs clustered. The previous 1000 pairs took 1.55 minutes.\n",
      "1,127,000 pairs clustered. The previous 1000 pairs took 1.41 minutes.\n",
      "1,128,000 pairs clustered. The previous 1000 pairs took 1.61 minutes.\n",
      "1,129,000 pairs clustered. The previous 1000 pairs took 1.47 minutes.\n",
      "1,130,000 pairs clustered. The previous 1000 pairs took 1.42 minutes.\n",
      "1,131,000 pairs clustered. The previous 1000 pairs took 1.35 minutes.\n",
      "1,132,000 pairs clustered. The previous 1000 pairs took 1.46 minutes.\n",
      "1,133,000 pairs clustered. The previous 1000 pairs took 1.36 minutes.\n",
      "1,134,000 pairs clustered. The previous 1000 pairs took 1.35 minutes.\n",
      "1,135,000 pairs clustered. The previous 1000 pairs took 1.37 minutes.\n",
      "1,136,000 pairs clustered. The previous 1000 pairs took 1.53 minutes.\n",
      "1,137,000 pairs clustered. The previous 1000 pairs took 1.38 minutes.\n",
      "1,138,000 pairs clustered. The previous 1000 pairs took 3.54 minutes.\n",
      "1,139,000 pairs clustered. The previous 1000 pairs took 1.22 minutes.\n",
      "1,140,000 pairs clustered. The previous 1000 pairs took 1.21 minutes.\n",
      "1,141,000 pairs clustered. The previous 1000 pairs took 1.24 minutes.\n",
      "1,142,000 pairs clustered. The previous 1000 pairs took 1.21 minutes.\n",
      "1,143,000 pairs clustered. The previous 1000 pairs took 1.19 minutes.\n",
      "1,144,000 pairs clustered. The previous 1000 pairs took 1.21 minutes.\n",
      "1,145,000 pairs clustered. The previous 1000 pairs took 1.43 minutes.\n",
      "1,146,000 pairs clustered. The previous 1000 pairs took 1.19 minutes.\n",
      "1,147,000 pairs clustered. The previous 1000 pairs took 1.22 minutes.\n",
      "1,148,000 pairs clustered. The previous 1000 pairs took 1.18 minutes.\n",
      "1,149,000 pairs clustered. The previous 1000 pairs took 1.36 minutes.\n",
      "1,150,000 pairs clustered. The previous 1000 pairs took 1.22 minutes.\n",
      "1,151,000 pairs clustered. The previous 1000 pairs took 1.30 minutes.\n",
      "1,152,000 pairs clustered. The previous 1000 pairs took 1.30 minutes.\n",
      "1,153,000 pairs clustered. The previous 1000 pairs took 1.38 minutes.\n",
      "1,154,000 pairs clustered. The previous 1000 pairs took 1.29 minutes.\n",
      "1,155,000 pairs clustered. The previous 1000 pairs took 1.34 minutes.\n",
      "1,158,000 pairs clustered. The previous 1000 pairs took 1.30 minutes.\n",
      "1,159,000 pairs clustered. The previous 1000 pairs took 1.49 minutes.\n",
      "1,160,000 pairs clustered. The previous 1000 pairs took 1.36 minutes.\n",
      "1,161,000 pairs clustered. The previous 1000 pairs took 1.28 minutes.\n",
      "1,162,000 pairs clustered. The previous 1000 pairs took 1.31 minutes.\n",
      "1,163,000 pairs clustered. The previous 1000 pairs took 1.34 minutes.\n",
      "1,164,000 pairs clustered. The previous 1000 pairs took 2.47 minutes.\n",
      "1,165,000 pairs clustered. The previous 1000 pairs took 1.27 minutes.\n",
      "1,166,000 pairs clustered. The previous 1000 pairs took 1.28 minutes.\n",
      "1,167,000 pairs clustered. The previous 1000 pairs took 1.72 minutes.\n",
      "1,168,000 pairs clustered. The previous 1000 pairs took 1.41 minutes.\n",
      "1,169,000 pairs clustered. The previous 1000 pairs took 1.34 minutes.\n",
      "1,171,000 pairs clustered. The previous 1000 pairs took 1.38 minutes.\n",
      "1,172,000 pairs clustered. The previous 1000 pairs took 1.33 minutes.\n",
      "1,173,000 pairs clustered. The previous 1000 pairs took 1.33 minutes.\n",
      "1,174,000 pairs clustered. The previous 1000 pairs took 1.38 minutes.\n",
      "1,175,000 pairs clustered. The previous 1000 pairs took 1.48 minutes.\n",
      "1,176,000 pairs clustered. The previous 1000 pairs took 1.32 minutes.\n",
      "1,177,000 pairs clustered. The previous 1000 pairs took 1.42 minutes.\n",
      "1,178,000 pairs clustered. The previous 1000 pairs took 2.43 minutes.\n",
      "1,179,000 pairs clustered. The previous 1000 pairs took 1.23 minutes.\n",
      "1,181,000 pairs clustered. The previous 1000 pairs took 1.98 minutes.\n",
      "1,182,000 pairs clustered. The previous 1000 pairs took 1.29 minutes.\n",
      "1,183,000 pairs clustered. The previous 1000 pairs took 1.33 minutes.\n",
      "1,184,000 pairs clustered. The previous 1000 pairs took 1.36 minutes.\n",
      "1,185,000 pairs clustered. The previous 1000 pairs took 1.32 minutes.\n",
      "1,186,000 pairs clustered. The previous 1000 pairs took 1.82 minutes.\n",
      "1,187,000 pairs clustered. The previous 1000 pairs took 2.49 minutes.\n",
      "1,188,000 pairs clustered. The previous 1000 pairs took 1.33 minutes.\n",
      "1,190,000 pairs clustered. The previous 1000 pairs took 1.30 minutes.\n",
      "1,191,000 pairs clustered. The previous 1000 pairs took 1.32 minutes.\n",
      "1,192,000 pairs clustered. The previous 1000 pairs took 1.32 minutes.\n",
      "1,193,000 pairs clustered. The previous 1000 pairs took 1.33 minutes.\n",
      "1,194,000 pairs clustered. The previous 1000 pairs took 4.89 minutes.\n",
      "1,195,000 pairs clustered. The previous 1000 pairs took 1.29 minutes.\n",
      "1,196,000 pairs clustered. The previous 1000 pairs took 1.27 minutes.\n",
      "1,197,000 pairs clustered. The previous 1000 pairs took 1.37 minutes.\n",
      "1,198,000 pairs clustered. The previous 1000 pairs took 1.35 minutes.\n",
      "1,199,000 pairs clustered. The previous 1000 pairs took 1.41 minutes.\n",
      "1,200,000 pairs clustered. The previous 1000 pairs took 2.40 minutes.\n",
      "1,201,000 pairs clustered. The previous 1000 pairs took 1.34 minutes.\n",
      "1,202,000 pairs clustered. The previous 1000 pairs took 1.30 minutes.\n",
      "1,203,000 pairs clustered. The previous 1000 pairs took 1.28 minutes.\n",
      "1,204,000 pairs clustered. The previous 1000 pairs took 1.34 minutes.\n",
      "1,205,000 pairs clustered. The previous 1000 pairs took 1.28 minutes.\n",
      "1,206,000 pairs clustered. The previous 1000 pairs took 1.32 minutes.\n",
      "1,207,000 pairs clustered. The previous 1000 pairs took 1.32 minutes.\n",
      "1,208,000 pairs clustered. The previous 1000 pairs took 1.35 minutes.\n",
      "1,209,000 pairs clustered. The previous 1000 pairs took 1.34 minutes.\n",
      "1,210,000 pairs clustered. The previous 1000 pairs took 1.32 minutes.\n",
      "1,211,000 pairs clustered. The previous 1000 pairs took 1.32 minutes.\n",
      "1,212,000 pairs clustered. The previous 1000 pairs took 1.33 minutes.\n",
      "1,213,000 pairs clustered. The previous 1000 pairs took 1.33 minutes.\n",
      "1,214,000 pairs clustered. The previous 1000 pairs took 1.39 minutes.\n",
      "1,215,000 pairs clustered. The previous 1000 pairs took 1.32 minutes.\n",
      "1,216,000 pairs clustered. The previous 1000 pairs took 1.29 minutes.\n",
      "1,217,000 pairs clustered. The previous 1000 pairs took 1.29 minutes.\n",
      "1,218,000 pairs clustered. The previous 1000 pairs took 1.29 minutes.\n",
      "1,220,000 pairs clustered. The previous 1000 pairs took 1.41 minutes.\n",
      "1,221,000 pairs clustered. The previous 1000 pairs took 1.32 minutes.\n",
      "1,222,000 pairs clustered. The previous 1000 pairs took 1.33 minutes.\n",
      "1,223,000 pairs clustered. The previous 1000 pairs took 1.41 minutes.\n",
      "1,224,000 pairs clustered. The previous 1000 pairs took 1.34 minutes.\n",
      "1,225,000 pairs clustered. The previous 1000 pairs took 1.35 minutes.\n",
      "1,226,000 pairs clustered. The previous 1000 pairs took 1.35 minutes.\n",
      "1,227,000 pairs clustered. The previous 1000 pairs took 1.35 minutes.\n",
      "1,228,000 pairs clustered. The previous 1000 pairs took 1.34 minutes.\n",
      "1,229,000 pairs clustered. The previous 1000 pairs took 1.34 minutes.\n",
      "1,231,000 pairs clustered. The previous 1000 pairs took 1.30 minutes.\n",
      "1,232,000 pairs clustered. The previous 1000 pairs took 1.36 minutes.\n",
      "1,233,000 pairs clustered. The previous 1000 pairs took 1.29 minutes.\n",
      "1,234,000 pairs clustered. The previous 1000 pairs took 1.32 minutes.\n",
      "1,235,000 pairs clustered. The previous 1000 pairs took 1.31 minutes.\n",
      "1,236,000 pairs clustered. The previous 1000 pairs took 1.35 minutes.\n",
      "1,237,000 pairs clustered. The previous 1000 pairs took 1.34 minutes.\n",
      "1,238,000 pairs clustered. The previous 1000 pairs took 1.37 minutes.\n",
      "1,239,000 pairs clustered. The previous 1000 pairs took 2.14 minutes.\n",
      "1,240,000 pairs clustered. The previous 1000 pairs took 1.36 minutes.\n",
      "1,241,000 pairs clustered. The previous 1000 pairs took 1.38 minutes.\n",
      "1,242,000 pairs clustered. The previous 1000 pairs took 1.36 minutes.\n",
      "1,243,000 pairs clustered. The previous 1000 pairs took 1.51 minutes.\n",
      "1,244,000 pairs clustered. The previous 1000 pairs took 1.36 minutes.\n",
      "1,245,000 pairs clustered. The previous 1000 pairs took 1.41 minutes.\n",
      "1,246,000 pairs clustered. The previous 1000 pairs took 1.40 minutes.\n",
      "1,247,000 pairs clustered. The previous 1000 pairs took 1.76 minutes.\n",
      "1,249,000 pairs clustered. The previous 1000 pairs took 1.35 minutes.\n",
      "1,250,000 pairs clustered. The previous 1000 pairs took 1.38 minutes.\n",
      "1,251,000 pairs clustered. The previous 1000 pairs took 1.36 minutes.\n",
      "1,252,000 pairs clustered. The previous 1000 pairs took 1.38 minutes.\n",
      "1,253,000 pairs clustered. The previous 1000 pairs took 1.37 minutes.\n",
      "1,254,000 pairs clustered. The previous 1000 pairs took 1.81 minutes.\n",
      "1,255,000 pairs clustered. The previous 1000 pairs took 1.40 minutes.\n",
      "1,256,000 pairs clustered. The previous 1000 pairs took 1.34 minutes.\n",
      "1,257,000 pairs clustered. The previous 1000 pairs took 1.47 minutes.\n",
      "1,258,000 pairs clustered. The previous 1000 pairs took 1.33 minutes.\n",
      "1,259,000 pairs clustered. The previous 1000 pairs took 1.36 minutes.\n",
      "1,260,000 pairs clustered. The previous 1000 pairs took 1.34 minutes.\n",
      "1,261,000 pairs clustered. The previous 1000 pairs took 1.39 minutes.\n",
      "1,262,000 pairs clustered. The previous 1000 pairs took 1.34 minutes.\n",
      "1,263,000 pairs clustered. The previous 1000 pairs took 1.34 minutes.\n",
      "1,264,000 pairs clustered. The previous 1000 pairs took 1.32 minutes.\n",
      "1,265,000 pairs clustered. The previous 1000 pairs took 1.46 minutes.\n",
      "1,267,000 pairs clustered. The previous 1000 pairs took 1.67 minutes.\n",
      "1,268,000 pairs clustered. The previous 1000 pairs took 1.33 minutes.\n",
      "1,269,000 pairs clustered. The previous 1000 pairs took 1.36 minutes.\n",
      "1,270,000 pairs clustered. The previous 1000 pairs took 1.34 minutes.\n",
      "1,271,000 pairs clustered. The previous 1000 pairs took 1.37 minutes.\n",
      "1,272,000 pairs clustered. The previous 1000 pairs took 1.34 minutes.\n",
      "1,273,000 pairs clustered. The previous 1000 pairs took 1.34 minutes.\n",
      "1,274,000 pairs clustered. The previous 1000 pairs took 1.35 minutes.\n",
      "1,275,000 pairs clustered. The previous 1000 pairs took 1.34 minutes.\n",
      "1,276,000 pairs clustered. The previous 1000 pairs took 1.31 minutes.\n",
      "1,277,000 pairs clustered. The previous 1000 pairs took 1.31 minutes.\n",
      "1,278,000 pairs clustered. The previous 1000 pairs took 1.33 minutes.\n",
      "1,279,000 pairs clustered. The previous 1000 pairs took 1.37 minutes.\n",
      "1,280,000 pairs clustered. The previous 1000 pairs took 1.29 minutes.\n",
      "1,281,000 pairs clustered. The previous 1000 pairs took 1.40 minutes.\n",
      "1,282,000 pairs clustered. The previous 1000 pairs took 1.40 minutes.\n",
      "1,283,000 pairs clustered. The previous 1000 pairs took 1.60 minutes.\n",
      "1,284,000 pairs clustered. The previous 1000 pairs took 1.69 minutes.\n",
      "1,285,000 pairs clustered. The previous 1000 pairs took 1.51 minutes.\n",
      "1,286,000 pairs clustered. The previous 1000 pairs took 1.44 minutes.\n",
      "1,287,000 pairs clustered. The previous 1000 pairs took 1.52 minutes.\n",
      "1,289,000 pairs clustered. The previous 1000 pairs took 1.52 minutes.\n",
      "1,290,000 pairs clustered. The previous 1000 pairs took 1.44 minutes.\n",
      "1,291,000 pairs clustered. The previous 1000 pairs took 1.65 minutes.\n",
      "1,292,000 pairs clustered. The previous 1000 pairs took 1.49 minutes.\n",
      "1,293,000 pairs clustered. The previous 1000 pairs took 1.48 minutes.\n",
      "1,294,000 pairs clustered. The previous 1000 pairs took 1.46 minutes.\n",
      "1,295,000 pairs clustered. The previous 1000 pairs took 1.90 minutes.\n",
      "1,296,000 pairs clustered. The previous 1000 pairs took 1.63 minutes.\n",
      "1,297,000 pairs clustered. The previous 1000 pairs took 1.60 minutes.\n",
      "1,298,000 pairs clustered. The previous 1000 pairs took 1.46 minutes.\n",
      "1,299,000 pairs clustered. The previous 1000 pairs took 1.44 minutes.\n",
      "1,300,000 pairs clustered. The previous 1000 pairs took 1.48 minutes.\n",
      "1,301,000 pairs clustered. The previous 1000 pairs took 1.47 minutes.\n",
      "1,302,000 pairs clustered. The previous 1000 pairs took 1.47 minutes.\n",
      "1,303,000 pairs clustered. The previous 1000 pairs took 1.47 minutes.\n",
      "1,304,000 pairs clustered. The previous 1000 pairs took 1.72 minutes.\n",
      "1,305,000 pairs clustered. The previous 1000 pairs took 1.50 minutes.\n",
      "1,306,000 pairs clustered. The previous 1000 pairs took 1.52 minutes.\n",
      "1,307,000 pairs clustered. The previous 1000 pairs took 1.52 minutes.\n",
      "Total time to cluster dates: 20.29 hours\n"
     ]
    }
   ],
   "source": [
    "exports_lf = cluster_dates(exports_lf, direction='export');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imports_lf.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write to Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write imports to clean parquet\n",
    "\n",
    "#get years\n",
    "years = pl.arange(2005,2025, eager=True)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for year in years:\n",
    "    print('Collecting {} dataframe...'.format(year))\n",
    "    df = (\n",
    "        imports_lf\n",
    "        .filter(pl.col('year')==year)\n",
    "        .collect()\n",
    "    )\n",
    "    print('Writing {} data to parquet...'.format(year))\n",
    "    df.write_parquet(file='data/imports/imports_'+str(year)+'.parquet')\n",
    "print('Imports data written to parquet')\n",
    "runtime = time.time() - start\n",
    "print('Total time to write imports: {:.2f} hours'.format(runtime/3600))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting exports data...\n",
      "Writing exports data to parquet...\n",
      "Exports data written to parquet.\n",
      "Total time to write exports: 0.02 hours\n"
     ]
    }
   ],
   "source": [
    "#write exports to clean parquet\n",
    "start = time.time()\n",
    "print('Collecting exports data...')\n",
    "df = exports_lf.collect()\n",
    "print('Writing exports data to parquet...')\n",
    "df.write_parquet('data/exports/exports.parquet')\n",
    "del df\n",
    "print('Exports data written to parquet.')\n",
    "runtime = time.time() - start\n",
    "print('Total time to write exports: {:.2f} hours'.format(runtime/3600))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limitations\n",
    "\n",
    "- voyage identification\n",
    "    - Ideally, a voyage would be defined by a vessel visiting a set of departure ports (e.g. Hong Kong, Beijing, and Tokyo) to pick up cargo and then delivering them to a set of arrival ports (e.g. Seattle, San Francisco, and Los Angeles). This way, we could measure the relative volumnes from each carrier actually on the ship during the main transit rather than only the relative volumes related to the specific lane. \n",
    "- missing data\n",
    "    - volume data missing from the first half of the dataset is problematic, and our current method of filling with the mean links volumnes to the number of BOLs. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wsu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
